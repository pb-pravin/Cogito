\documentclass[a4paper,oneside]{report}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[english]{babel}
\usepackage{fancyhdr} 
\usepackage[pdftex]{graphicx}
\usepackage{listings}      
\usepackage{pdfpages}
\usepackage{setspace}
\usepackage{url}

\makeatletter



%
% Some custom definitions
%

% add horizontal lines
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\HRuleLight}{\rule{\linewidth}{0.1mm}}

% custom part page
\def\part#1#2
{
	\par\break
  	\addcontentsline{toc}{part}{#1}
	\noindent
	\null	
	\HRuleLight\\[0.0cm]
	\vspace{20pt}	 
	\begin{flushright} 		
  	{\Huge \bfseries \noindent #1}\\
  	\vspace{30pt} 
	\begin{minipage}{0.85\textwidth}
		\begin{flushright}
		{\large \noindent #2}
		\end{flushright}
	\end{minipage}\\[0.75cm] 
	\end{flushright} 		
	\thispagestyle{empty}
	\break
}

% chapter header
\renewcommand{\@makechapterhead}[1]
{\vspace*{50\p@}{
	\parindent \z@ \raggedright \normalfont	%\huge \bfseries \thechapter. #1
	\huge \bfseries #1	\vspace{20pt}}}

\setcounter{secnumdepth}{-1} 
\onehalfspace
\oddsidemargin 1in 
\oddsidemargin 0.6in 
\topmargin -0.3in
\setlength{\textwidth}{14cm}
\setlength{\textheight}{23cm}

\begin{document}

%
% Cover page
%
\begin{titlepage}
\begin{center}

\includegraphics[width=120mm]{sources/images/cogito_logo_main.png}

\HRuleLight\\[0.5cm]

\begin{minipage}{0.45\textwidth}
	\begin{flushleft}\large
		\emph{Author:}\\
			\textbf{Thomas \textsc{Taylor}}\\[0.27cm]
			Computer Science (Games)
			Student Number: 08813043
	\end{flushleft}
\end{minipage}
\begin{minipage}{0.43\textwidth}
	\begin{flushright} \large
		\emph{Supervisor:} \\
		\textbf{Graham \textsc{Winstanley}}\\[0.25cm]
		\emph{Second Reader:}\\
		\textbf{Saeed \textsc{Malekshahi}}
	\end{flushright}
\end{minipage}\\[0.75cm] 

\HRuleLight\\[0.2cm]

\large School of Computing, Engineering and Mathematics\\ \textbf{University of Brighton}

\vfill
\huge Project Documentation\\
\large April, 2012\\

\end{center}
\end{titlepage}



%
% Table of contents
%
{
	\renewcommand\thepage{}
	\setcounter{tocdepth}{1}
	\tableofcontents
	\clearpage
}

% reset page count
\setcounter{page}{1}


%
% Start of content
%

\chapter{Introduction}

The artificial intelligence (AI) techniques utilised by game developers have been called archaic by the academic AI researchers, which may indeed be true. However, it is important to view such techniques in context of which they are used. While academic AI researchers are able to utilise as much processing power as the hardware can afford them, game developers are much more limited. When one examines the other processes which need to run concurrently, this is not overly surprising: physics, sound, networking, input and most importantly the graphics processing to name but a few. So when AI researchers complain that the techniques are archaic, which is a fair arguement, given that the A* search algorithm is still one of the most popular AI algorithms used in games today, and was first introduced over 30 years ago), this is for the simple reason that the amount of processing power that developers are able to dedicate to the AI processes could not cope with any more `recent', processor intensive techniques. Indeed, modern game AI programming is more a skill of employing the cheapest (computationally) techniques, and often means that shortcuts and workarounds are a necessity in order to achieve `believable' AI. 

However, this being said, we are starting to see a change taking place in the computer games industry which suggests that the processing time availiable for often overlooked processes such as AI is starting to increase. One such factor in this change is the so-called `graphics plateau' \cite{Sheffield:2008fk}. Whether or not there will come a point where the graphical complexity of a game is considered `perfect', with no area for improvement is uncertain (and in my opinion highly unlikely). However, I do think that we have reached a point whereby the graphical complexity achievable with each new generation in games consoles is unlikely to achieve the same kind of drastic improvement in detail that we have seen over the last couple of decades; in effect, I think that we have reached a level of complexity whereby the amount of processing power needed for the graphics processing is unlikely to increase by a very large factor. Given the inevitable hardware advances that come with each new generation, I believe that we will start see a lot more processing power freed up for those processes which have been treated as `extraneous' in the past, such as the AI. 

Another important factor in this change is the emergence of the `casual' gamer. With casual games now accounting for a considerable proportion game sales, the consumer has shown that there is a big market for games which have less emphasis on complex graphics effects, and more emphasis on interesting gameplay mechanics \cite{Association:2011uq}. Indeed, with Nintendo's Wii home console having far outsold both Sony's PlayStation 3 and Microsoft's Xbox, the consumer has shown that they value unique gameplay and accessibility over advanced graphics \cite{:2012dq, Nintendo:2012nx, :cr}. 

\paragraph{USER BASE PIE CHART}\ \\ \\

With these factors already starting to bring about a change in the priority that developers give to the components of their games, I believe that now is as good a time as any for developers to introduce some more advanced AI techniques into their games.

\section{Project Aims and Objectives}

My main aim with my project was to experiment with some more advanced academic AI techniques which are not typically found in commercial games today for various reasons, and look into whether such techniques were actually viable with today's hardware. This second point is key, especially given the popularity of fairly limited hardware such as the Nintendo Wii, and mobile hardware.

The sub-field of AI that I chose to focus my project on was machine learning, as it is an emerging field which greatly interests me and one which I feel is a natural fit for computer games, and one we could see utilised in many forms in future games.

For my project, I wanted to develop an AI system which was capable of employing machine learning techniques in order for a number of AI-controlled agents to safely navigate a 2D game environment. I wanted the agents to be able to develop a knowledgebase dynamically, with no need for prior training, or knowledge of the game world. I then wanted the agents to be able to apply this gained knowledge in order to hopefully increase their chances of survival. 

\section{Background}

I chose the classic Commodore Amiga game `Lemmings' as the context and main inspiration for my project.

\begin{figure}[h!]
  \centering
    \includegraphics[width=80mm]{sources/images/lemmings3}
    \caption{A typical level in Lemmings.\label{screen}}
\end{figure}

Originally released in 1991 for the PC and Commodore Amiga, `Lemmings' has a very simple premise: to guide a group of computer-controlled `lemmings’ across a level from the entrance-point to the exit. The Lemming characters themselves have very basic AI; capable of merely walking across the level map. The level itself consists of a number of platforms, along with a number of hazards which will kill the Lemmings (big drops, pits etc.). The goal of the player is to utilise a set of tools in order to ensure that the Lemmings safely reach the level exit, such as umbrellas for big drops, girders to cross pits etc. 

For my project, I planned to take this concept, simplify it by removing some of the tools and obstacles, and program some AI agents to learn for themselves how to best traverse the level, without any human interation.
		
\section{The Project in Context}
	
Throughout the development of my project, I needed to utilise skills which I had gained from modules I had studied during my course.

There was obviously a lot of knowledge that I gained in the AI-related modules I have studied (CI213 - Intelligent Systems and CI342 - Advanced AI) which I could directly apply to my project. Techniques such as pathfinding algorithms, and generally approaching AI-based problems from an academic angle.

I was also able to apply a lot of useful knowledge that I had gained from the programming-based modules from my course (CI101 - Introduction to Programming, CI228 - Object-Oriented Software Design and Specification and CI346 - Programming, Concurrency and Client-Server Computing). I applied a lot of the `good' object-oriented design practices which were learnt in these modules: principles such as inheritance and design patterns like the singleton. I also found the knowledge learnt this year in CI346 very useful to overcome some concurrency issues I had using multiple threads in my system.

Another module I have found to be very useful is CI224 (Games Development). Many of the game design concepts: physics, object transformations, model loading, and various other useful skills. The project we did in CI224 was particularly useful, as it introduced me to the process involved in developing a game from scratch. I was also introduced to source control systems in CI224, which has proven to be invaluable knowledge to have.

I also used the knowledge of object-oriented software design and UML learnt in CI228 and CI231 (Formal Underpinnings and Specification) to design my system. Although I didn't actually use the formal specification language learnt in my system specification, I could still apply the concepts learnt to my own designs.

I learnt a great deal about data access and performance optimisation in the CI312 (Computer Graphics Algorithms) module. It really helped me too look at my code at a lower level, and analyse the best and most efficient ways to access and manipulate data; something which I have found to be incredibly important, especially in a game context, where some functions can be called 60 times every second. This is something which is of paramount importance, especially in games, as you will likely be carrying out certain functions up to 60 times every second, so any inefficient code would cause big problems in performance. 



%
% New part
%

\part{Research}{This section documents the research that I carried out prior to designing my system, and how it influenced my final designs. \\ \ \\The section is split into two subsections: my research into AI, and my research into the more game-related topics.}

\chapter{Artificial Intelligence}

Since the early days of `modern' computing, computer scientists have striven to replicate in machines the defining characteristic that makes us human: intelligence. One of the first references to `intelligent machines' came in 1950 in Alan Turing's seminal paper \emph{Computing Machinery and Intelligence}, in which Turing opens with the words: "I propose to consider the question, 'Can machines think?'". In it, he outlines a test in which a human participant engages in conversation with a machine designed to imitate human behaviour, known as the Turing Test. This paper not only inspired the concept of `intelligent machines', but also caused us to look philosophically at what it is that makes us inherently human, and whether this is indeed replicable in a machine.

The term `artificial intelligence' was later formally defined in 1955 by McCarthy et al in the proposal for the Dartmouth Summer Research Conference on Artificial Intelligence, which is considered by many to be the origin of the field of AI \cite{Crevier:1993kl}. In the proposal, it was suggested that ``every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it" \cite{McCarthy:1955ve}. However, despite this early optimism, we are still yet to see any truly `intelligent' machines nearly 60 years on. Indeed, the annual Loebner Prize event in which AI researchers partake in Turing Tests has yet to award the silver or gold medals to any entrant. 

Since these very early days, the field of AI has grown to become an integral field not only in computer science, but also in the infrastructure of every industry \cite{Kurzweil:2005ly}. AI techniques are used in a vast array of application areas, such as in search, elevator scheduling, satellite monitoring, weather prediction, aeroplane autopiloting and robotics to name just a very few. 

\section{AI In Games}

One area where AI techniques are used extensively is computer games. Whether to map a route across obstacle-ridden terrain, control the actions of non-player characters (NPCs) or even to adjust game parameters to fit to the skill of the player, AI plays a key role in almost all modern computer games. 

However, despite the proven benefits of AI, the amount of system resources allocated to AI processing is surprisingly limited; traditionally, the game industry has been much more focussed on producing ultra-realistic visuals than with developing a truly intelligent AI system. With each new console generation, developers have pushed the hardware to its limits, offering greater levels of visual fidelity while other areas of development were neglected. It is no surprise then, that game AI programmers have had to make the best of a bad situation, often being forced to avoid  more powerful and resource-intensive AI techniques for a mixture of highly optimised (and less-effective) techniques, and a `smoke and mirrors' approach.

Due to the nature of the final product, game AI takes a very contrasting approach to that of academic AI. While the aim with academic AI systems is always to provide the most complete and accurate results, the ultimate aim with game AI is to provide entertainment for the user, by giving the illusion of intelligence. Ironically, this can often mean that it is necessary to make the system intentionally `stupid' by building flaws in the system or even by allowing the system to cheat to ensure that the game AI behaves `realistically' \cite{Liden:2004fk}. Additionally, academic AI systems usually have the very simplistic interfaces or visualisation, in order to allow the majority of processing time to be dedicated to the AI. In a computer game on the other hand, the AI must run simultaneously with a number of other processes such as the physics engine, sound and most importantly graphics which in reality means that the AI processing is often compromised in the favour of other processes.

The AI systems used in most modern games mainly concern themselves with the following three objectives \cite{millington2006artificial}:

\begin{itemize}
	\item The ability to move characters
	\item The ability to make decisions about where to move
	\item The ability to think tactically or strategically
\end{itemize}

In the proceeding sections, I outline some of the most popular AI techniques used in commercial computer games today.

\subsection{Finite State Machines} 

Finite state machines (FSMs) are one of the most basic and most commonly used AI techniques found in games today. As the name suggests, finite state machines are ``simple, rule-based systems in which a finite number of 'states' are connected in a directed graph by 'transitions' between states" \cite{:hc}. This graph essentially maps the states available, and which of these states can be reached from the current state. One of the first games to sucessfully implement FSMs is Atari's 1979 game \emph{Pacman}, in which the enemy ghost characters use a FSM to either roam around, chase Pacman or run away, changing their state in response to various in-game triggers. Despite the age of this technique, it is still used extensively in many commercial games today, due to the fact that FSMs are fairly easy to understand, easily implementable, and most importantly, relatively straightforward to debug \cite{Bourg:2004tg}.

\subsection{Search and Planning Systems} 

Search and planning systems have many different uses and are used extensively in many different genres of games. Search systems are primarily concerned with finding actions or states in a graph in order to satisfy a goal condition (or at least to get as close as possible to the goal condition). Planning systems are a branch of the search methods which have an emphasis on finding the best (or simplest) sequence of actions required in order to reach a goal state. There have been a variety of search algorithms developed over the years, but the A* algorithm is the one used most extensively, due to its performance, and the accuracy of its results. Search and planning systems are used for a variety of tasks, from the pathfinding of AI agents in a first-person shooter game to the allocation of resources in a real-time strategy game. Blizzard's 1994 game \emph{Warcraft} is an example of an early implementation of using the A* pathfinding algorithm to navigate agents.

\subsection{Artificial Life} 

The use of artificial life (A-life) techniques have become more commonplace in recent years, and have a variety of different uses. An A-life system is essentially system which is designed to exhibit realistic and lifelike behavior in game characters. A-life systems are often used to coordinate the movement of multi-agent systems to make the agents maneuver in flock and herd formation. One of the most compelling reasons to use A-life techniques is that more complex intelligent behaviours can emerge as a result from the pre-programmed rules \cite{Woodcock:bs}. \emph{The Sims} developed by Maxis (now part of EA) is probably one of the best known games to use A-life techniques, and uses so-called `smart terrain' to broadcast information to the game characters. For example,  a character which is hungry may walk past a refridgerator, which broadcasts that it contains food. After collecting the food, the nearby oven may broadcast that it can cook the food, leading the character to it \cite{Woodcock:bs}. These individual object-level actions when combined, cause the character to act realistically. While A-life techniques can result in some very realistic and emergent behaviour without having being explicitly programmed to do so, it also means that the unpredictability which is inherent to A-life can mean that it can be incredibly difficult to debug. Additionally, the more complex techniques can be incredibly processor intensive, rendering them unsuitable for games which are already very processor-intensive. 

\subsection{Scripting} 

Another popular technique in game AI is scripting, whereby developers use a specialised high-level scripting language to outline the NPCs reactions to basic situations as well as to hard-code game events. Scripting allows the game engine to be accessed externally in a safe `sandbox' environment, greatly reducing the chance of bugs and errors, so is suitable for non-programmers such as designers, or even end-users to access. For this reason, there is a large community of gamers which build additional content for various games via the scripting interface. Common uses for scripting include creating events and controlling the enemy AI. 

Lionhead used scripting heavily in their 2001 game \emph{Black \& White} to present the game's storyline using a series of in-game challenges rather than the more traditional cut-scene, as well as to implement the game's logic \cite{:hc}. 

A big disadvantage to using scripting is that it requires every character behaviour and game scenario to be hard-coded which is incredibly time consuming for the programmer/developer, and is not always possible. Additionally, using scripting can be very restrictive for the player, as the story progression is often very linear.

\subsection{Cheating The System}

In addition to the AI techniques outlined above, many developers also use a number of hacks and workarounds to give their AI systems the illusion of intelligence while retaining the entertainment value. In many cases, the more traditional AI techniques aren't needed at all. As discussed earlier, the primary aim when it comes to game AI is to make the AI system appear `intelligent' whilst at the same time presenting an acceptable level of challenge for the player. If we examine a stealth action game such as Konami's \emph{Metal Gear Solid} series in which the player must traverse a level while avoiding detection from the guard NPCs; were a truly intelligent and adaptive AI system used which was able to reason as completely as the player, it would remove a lot of the entertainment value of the game which often relies on the player recognising patterns in guard behaviour in order to progress. While this AI system would be considered flawed in the eyes of an academic researcher, it is perfectly acceptable in the context of the game. Similarly, game developers often give the AI system access to extra information or resources which are unavailable to the player. This is particularly the case with real-time strategy games (RTSs) where it is commonplace for the enemy player to have access to extra resources such as troops or supplies which wouldn't be available to the player. Using these kinds of `cheats' can be an acceptable way to increase the level of difficulty. However, the challenge for the developer is to disguise this from the player; if the player feels that the game is acting unfairly, and that their actions are ineffective, they will obviously not enjoy the game.

\subsection{The Future of Game AI}

From as early as 2000, statistics were showing that the industry was beginning to take AI more seriously. This was reflected in figures collected from the AI roundtable at the annual Game Developer's Conference (GDC), which showed that not only were a lot more studios employing dedicated AI programmers, but also the average amount of CPU time reserved for AI processing increased from 5\% to over 25\% \cite{Woodcock:oq}. With increased resources now available to AI developers, there is more opportunity now than ever to introduce newer and more advanced AI techniques such as neural networks and machine learning.

\section{Machine Learning}

The ability to learn is one of the central features of intelligence \cite{Langley:1996zr}. To be able to then apply this knowledge and modify behaviour accordingly when faced with similar scenarios in the future is something which has recieved a considerable amount of attention, with techniques having been developed to tackle a variety of learning problems. In fact, machine learning techniques are already in use in a number of commercial applications such as speech recognition, robotic control and machine vision, proving that it can be a very useful solution, especially in solving problems which may have unexpected outcomes that cannot be predicted by the software developer.

\noindent A popular definition of machine learning as phrased by respected machine learning researcher Tom Mitchell \cite{mitchell1997machine}: 
\begin{quotation}``A computer is said to learn from experience \emph{E} with respect to some class of tasks \emph{T} and performance measure \emph{P}, if its performance at tasks in \emph{T}, as measured by \emph{P}, improves with experience \emph{E}."
\end{quotation}

Although it may be fairly self explanatory, the major benefit of implementing a machine learning algorithm is that a system is able to draw upon data collected from experience, and analyse this data to improve its performance in the future.

\paragraph{} Learning techniques can be classified into various groups based on what is learnt, when learning occurs and the effects that the learning has on the system. 

\paragraph{Online and Offline:} Learning is said to be `online' if the system learns while it is actually running, and `offline' if the data is analysed as a whole and the system improved \emph{after} it has finished running. While online learning provides instantaneous results, it also makes the system much more difficult to debug were any issues found, due to its continous modification.

\paragraph{Supervised and Unsupervised:} Learning techniques can also be categorised by how they use training data, if at all. A system which uses training data to base its learning on is described as `supervised', whereas a system which only has access to the data that it collects itself is described as `unsupervised'. The benefit to supervised learning is that it gives the system some correct `solutions' to base its future learning on, leading to more accurate and perhaps generally `quicker' learning. However, the training data itself needs to be collected and analysed, with only `correct' data being transferred to the system. This obviously requires human input, and can be very time consuming. Unsupervised methods on the other hand do not have this requirement, but must analyse the collected data to find hidden structure; there is no `correctness' indicator to evaluate potential solutions. There are also techniques which form an intermediary between the two, using both labelled and unlabelled data. These methods are known as `semi-supervised'.

\paragraph{`Inter' and `Intra' Behaviour:} Learning techniques are also often classified by the amount that they change the system's behaviour \footnote[1]{The term `behaviour' here is used to describe ``a qualitatively different mode of action" }. The simpler techniques, which maybe only adjust some basic parameters of only change a small part of a system's behaviour are known as intra-benaviour, whereas methods which use inter-behaviour learning have a much bigger effect on a system's behaviour. For example, a game character which learns the most effective patrol route would be considered intra-behaviour, whereas a character which learns to lay an ambush for the player rather than to wait until the player appears, would be considered inter-behaviour.

\paragraph{} There have been a number of different machine learning techniques developed for 

\subsection{Reinforcement Learning}

For most applications, reinforcement learning falls into 3 categories: Dynamic Programming, Monte Carlo, and Temporal Difference.


Reinforcement learning is the name given to a range of techniques for learning based on experience. In its most general form a reinforcement learning algorithm has three components: an exploration strategy for trying out different actions in the game, a re- inforcement function that gives feedback on how good each action is, and a learning rule that links the two together. Each element has several different implementations and optimizations, depending on the application.Reinforcement learning is a hot topic in game AI, with more than one new AI middleware vendor using it as a key technology to enable next-generation gameplay.
Later in this section we’ll look briefly at a range of reinforcement learning tech- niques. In game applications, however, a good starting point is the Q-learning al- gorithm. Q-learning is simple to implement, has been widely tested on non-game applications, and can be tuned without a deep understanding of its theoretical prop- erties.7.6.1 THE PROBLEMWe would like a game character to select better actions over time. What makes a good action may be difficult to anticipate by the designers. It may depend on the way the player acts, or it may depend on the structure of random maps that can’t be designed for.We would like to be able to give a character free choice of any action in any cir- cumstance and for it to work out which actions are best for any given situation.Unfortunately, the quality of an action isn’t normally clear at the time the action is made. It is relatively easy to write an algorithm that gives good feedback when the character collects a power-up or kills an enemy. But the actual killing action may have been only 1 out of 100 actions that led to the result, with each one of which needing to be correctly placed in series.Therefore, we would like to be able to give very patchy information: to be able to give feedback only when something significant happens. The character should learn that all the actions leading up to the event are also good things to do, even though no feedback was given while it was doing them.7.6.2 THE ALGORITHMQ-learning relies on having the problem represented in a particular way. With this representation in place, it can store and update relevant information as it explores the possible actions it can take. We’ll look at the representation first.Q-Learning’s Representation of the WorldQ-learning treats the game world as a state machine. At any point in time, the al- gorithm is in some state. The state should encode all the relevant details about the character’s environment and internal data.So if the health of the character is significant to learning, and if the character finds itself in two identical situations with two different health levels, then it will consider them to be different states. Anything not included in the state cannot be learned. If we didn’t include the health value as part of the state, then we couldn’t possibly learn to take health into consideration in the decision making.In a game the states are made up of many factors: position, proximity of the en- emy, health level, and so on. Q-learning doesn’t need to understand the components7.6 Reinforcement Learning 613
614 Chapter 7 Learningof a state. As far as the algorithm is concerned they can just be an integer value: the state number.The game, on the other hand, needs to be able to translate the current state of the game into a single state number for the learning algorithm to use. Fortunately, the algorithm never requires the opposite: we don’t have to translate the state number back into game terms (as we did in the pathfinding algorithm, for example).Q-learning is known as a model-free algorithm because it doesn’t try to build a model of how the world works. It simply treats everything as states. Algorithms that are not model-free try to reconstruct what is happening in the game from the states that it visits. Model-free algorithms, such as Q-learning, tend to be significantly easier to implement.For each state, the algorithm needs to understand the actions that are available to it. In many games all actions are available at all times. For more complex environ- ments, however, some actions may only be available when the character is in a partic- ular place (e.g., pulling a lever), when they have a particular object (e.g., unlocking a door with a key), or when other actions have been properly carried out before (e.g., walking through the unlocked door).After the character carries out one action in the current state, the reinforcement function should give it feedback. Feedback can be positive or negative and is often zero if there is no clear indication as to how good the action was. Although there are no limits on the values that the function can return, it is common to assume they will be in the range [−1, 1].There is no requirement for the reinforcement value to be the same every time an action is carried out in a particular state. There may be other contextual information not used to create the algorithm’s state. As we saw previously, the algorithm cannot learn to take advantage of that context if it isn’t part of its state, but it will tolerate its effects and learn about the overall success of an action, rather than its success on just one attempt.After carrying out an action, the character is likely to enter a new state. Carrying out the same action in exactly the same state may not always lead to the same state of the game. Other characters and the player are also influencing the state of the game.For example, a character in an FPS is trying to find a health pack and avoid getting into a fight. The character is ducking behind a pillar. On the other side of the room, an enemy character is standing in the doorway looking around. So the current state of the character may correspond to in-room1, hidden, enemy-near, near-death. They chose the “hide” action to continue ducking. The enemy stays put, so the “hide” ac- tion leads back to the same state. So they chose the same action again. This time the enemy leaves, so the “hide” action now leads to another state, corresponding to in- room1, hidden, no-enemy, near-death.One of the powerful features of the Q-learning algorithm (and most other rein- forcement algorithms) is that it can cope with this kind of uncertainty.These four elements—the start state, the action taken, the reinforcement value, and the resulting state—are called the experience tuple, often written as ⟨s, a, r, s′⟩.
Doing LearningQ-learning is named for the set of quality information (Q-values) it holds about each possible state and action. The algorithm keeps a value for every state and action it has tried. The Q-value represents how good it thinks that action is to take when in that state.The experience tuple is split into two sections. The first two elements (the state and action) are used to look up a Q-value in the store. The second two elements (the reinforcement value and the new state) are used to update the Q-value based on how good the action was and how good it will be in the next state.The update is handled by the Q-learning rule:Q(s, a) = (1 − α)Q(s, a) + α􏳁r + γ max􏳁Q(s′, a′)􏳂􏳂,where α is the learning rate, and γ is the discount rate. Both are parameters of the algorithm. The rule is sometimes written in a slightly different form, with the (1 − α) multiplied out.How It WorksThe Q-learning rule blends together two components using the learning rate parame- ter to control the linear blend. The learning rate parameter, used to control the blend, is in the range [0, 1].The first component Q(s, a) is simply the current Q-value for the state and action. Keeping part of the current value in this way means we never throw away information we have previously discovered.The second component has two elements of its own. The r value is the new rein- forcement from the experience tuple. If the reinforcement rule wasQ(s,a)=(1−α)Q(s,a)+αrthen it would be blending the old Q-value with the new feedback on the action.The second element, γ max(Q(s′, a′)), looks at the new state from the experience tuple. It looks at all possible actions that could be taken from that state and chooses the highest corresponding Q-value. This helps bring the success (i.e., the Q-value) of a later action back to earlier actions: if the next state is a good one, then this stateshould share some of its glory.The discount parameter controls how much the Q-value of the current state andaction depends on the Q-value of the state it leads to. A very high discount will be a large attraction to good states, and a very low discount will only give value to states that are near to success. Discount rates should be in the range [0, 1]. A value greater than 1 can lead to ever-growing Q-values, and the learning algorithm will never con- verge on the best solution.7.6 Reinforcement Learning 615
616 Chapter 7 Learning￼PROGRAMSo, in summary, the Q-value is a blend between its current value and a new value, which combines the reinforcement for the action and the quality of the state the ac- tion led to.Exploration StrategySo far we’ve covered the reinforcement function, the learning rule, and the internal structure of the algorithm. We know how to update the learning from experience tuples and how to generate those experience tuples from states and actions. Rein- forcement learning systems also require an exploration strategy: a policy for selecting which actions to take in any given state. It is often simply called the policy.The exploration strategy isn’t strictly part of the Q-learning algorithm. Although the strategy outlined below is very commonly used in Q-learning, there are others with their own strengths and weaknesses. In game, a powerful alternative technique is to incorporate the actions of a player, generating experience tuples based on their play. I’ll return to this idea later in the section.The basic Q-learning exploration strategy is partially random. Most of the time, the algorithm will select the action with the highest Q-value from the current state. The remainder, the algorithm will select a random action. The degree of randomness can be controlled by a parameter.Convergence and EndingIf the problem always stays the same, and rewards are consistent (which they often aren’t if they rely on random events in the game), then the Q-values will eventu- ally converge. Further running of the learning algorithm will not change any of the Q-values. At this point the algorithm has learned the problem completely.For very small toy problems this is achievable in a few thousand iterations, but in real problems it can take a vast number of iterations. In a practical application of Q-learning, there won’t be nearly enough time to reach convergence, so the Q-values will be used before they have settled down. It is common to begin acting under the influence of the learned values before learning is complete.


Dynamic Programming
Dynamic programming is used to solve learning problems where state transition probabilities are known beforehand. The state transition probability function, p(s,a,s`) is the probability of transitioning from state s to state s` when the agent takes action a. The goal of the learning agent is to learn to approximate the reward function V(s,a), which gives the expected reward of taking action a in state s. The agent approximates the reward function by testing out different actions in different states, each time receiving a reward r. For taking action a in state s, the agent updates V(s,a) with the following function:

V(s,a) <= r + gamma * sum(p(s, a, s`) * V(s')) (Eq. 1)

Where gamma is a parameter known as the discount rate, the summation sums over all states s` and V(s`) represents the maximum V(s`,a) for all actions a. This equation is known as the Bellman equation.

Monte Carlo and Temporal Difference
Monte Carlo and Temporal Difference methods are used to solve problems where p(s,a,s`) is not known. Temporal Difference solves the learning equation by using the expected reward of the next state it encounters after taking an action a in state s. The full equation is:

V(s,a) <= V(s,a) + alpha * ( r + gamma * V(s`) - V(s,a) ) (Eq. 2)

Where alpha is a parameter known as the learning rate. Monte Carlo is similar to Temporal Difference but keeps track of all rewards encountered until an episode terminates in addition to the one state look-ahead performed by temporal difference methods.

As far as the original poster's question goes: reinforcement learning is used because in your problem, there will be many actions that have the possibility to succeed in any given state. Searching for a possible action will not necessarily retrieve the optimal action. That is where learning comes in. For wall following you will probably have a continuous state space and continuous action space, which will considerably complicate the problem.

Regarding other posts in this thread: remember that you are not required to post responses to questions in the forums. If you only have a very vague and shaky understanding of a topic, it is best to keep that vague and shaky answer to yourself. People expect accuracy when they ask questions on the forums, and the goal of gamedev is not to see who can post the most. 

Reinforcement learning is not learning by example, but from rewards.
 
The aim of reinforcement learning is to maximise the expected reward over time. 

Trade-off of long-term and short-term rewards. 

"One approach to the acquisition of problem-solving knowledge, reinforcement learning, focuses on preference knowledge for the selection of operators. Because this class of methods does not require information about operator effects, it is commonly applied to learning from sequences of external actions. However, many of the same issues arise in this context as with internal problem solving, and we will not typically distinguish between them.

In addition to a set of (possibly opaque) operators, a reinforcement learning system is provided with some reward function that evaluates states. Intuitively, states with higher rewards are more desirable than ones with low or negative rewards, and the agent aims to approach the former and avoid the latter. Learning involves the acquisition of a control strategy that leads to states with high rewards, based on experience with the rewards produced by observed operator sequences.

The Simplest approach to reinforcement learning stores knowledge in a table. Each entry describes a possible state-action pair, along with the expected reward that results from applying the action to that state."

"Typically, the problem solver uses this table to direct forward-chaining search. This involves finding the table entry for the current state, selecting the action that gives the highest expected score, and applying this action to produce a new state. The problem solver continues this cycle, producing a forward-chaining greedy search through the state space, until it reaches the desired state or some other halting condition."

How an agent ought to take actions in an environment so as to maximise the end reward. Needs no knowledge o the environment. Correct input/output pairs are never presented, not sub-optimal actions explicitly corrected. Finding a balance between exploration of need territory and exploitation of existing knowledge. Reinforcement learning is well suited to problems which include long-term vs. short-term trade-off. 

Possible applications for reinforcement learning:
-	Robot control
-	Elevator scheduling
-	Backgammon
-	Checkers

"Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs."

"When the agent's performance is compared to that of an agent which acts optimally from the beginning, the difference in performance gives rise to the notion of regret. Note that in order to act near optimally, the agent must reason about the long term consequences of its actions: In order to maximize my future income I better go to school now, although the immediate monetary reward associated with this might be negative.

Thus, reinforcement learning is particularly well suited to problems which include a long-term versus short-term reward trade-off."

1. Set of environment states
2. Set of actions
3. Rules of transitioning between states
4. Rules to determine the immediate reward
5. Rules to describe what the agent observes

An agent interacts with the environment in discrete time-steps. At each step, the agent receives an observation, and chooses an action from those available. The environment is updated, and the reward is calculated.

"Two components make reinforcement learning powerful: 
-	The use of samples to optimise performance 
-	The use of function approximation to deal with large environments

The only way to collect information about the environment is by interacting with it.

(could be considered a genuine learning problem)"

\subsubsection{Markov Decision Processes:}

When it comes to representing the 

Markov Decision Processes (MDPs) are used 



Markov decision processes (MDPs), named after Andrey Markov, provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). Much research in the area was spawned due to Ronald A. Howard's book, Dynamic Programming and Markov Processes, in 1960. Today they are used in a variety of areas, including robotics, automated control, economics and manufacturing.

More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action  that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward .

The probability that the process moves into its new state  is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state  depends on the current state  and the decision maker's action . But given  and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP possess the Markov property.

Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are zero, a Markov decision process reduces to a Markov chain.



\begin{quotation}A Markov Decision process (MDP) M is a 4-tuple (S,U,P,R), where S is a set of thestates, U is a set of actions (U(i) is the set of actions available at state i), PM(a) is the transition probability from state i to state j when performing action a ∈ U (i) in state i, and RM (s, a) is the reward received when performing action a in state s.\end{quotation}

\subsubsection{Algorithms:}

- Policy iteration
Heuristic algorithms act almost as an approximation; don't guarantee to find the optimal solution. Deterministic algorithms on the other hand perform predictably, giving the same result each time it's used. 
- Monte Carlo
Mimics policy iteration. 

- Temporal difference
- Bucket brigade
- Q-learning

"Reinforcement learning alters the predicted rewards stored int the state-action table on the basis of experience. The entries typically start with either random values, or equal ones, but the score for a given state-action pair <s,a> changes whenever the problem solver applies action a in state s. This produces a new state and a resulting reward, which the learner uses to revise the existing entry."

"This strategy, known as Q-learning, computes the sum of the immediate reward and the discounted maximum reward expected for the following state, then subtracts the current entry for the state-action pair. It then multiplies the difference by the correction rate and modifies the entry in question by the resulting product. The discount factor specifies the the importance of immediate versus delayed gratification, whereas the correction factor determines the rate at which the learner revises table entries."

"Given enough training cases, this learning scheme converges on the expression Q(s,a) = r(s,a) + discount factor(U(s prime)) as the entry for each state-action pair <s,a> in the table."

"Thus, after sufficient experience, the problem solver can move toward the most desirable state from any location with little to no search"

\begin{itemize}
	\item Discuss convergence (Q-learning converges with probability 1)
	\item Learning rates
	\item Discount factors
\end{itemize}

\subsection{Decision Tree}

Pros:
\begin{itemize}
	\item Relatively easy to use and understand; decision trees are easy to visualise and read
	\item Tree structures are a very commonly used data structure in computer science, and highly optimised algorithms for the building and searching of trees have already been developed
\end{itemize}

\noindent Cons:
\begin{itemize}
	\item Can lack accuracy; the size of the tree will increase tremendously to provide a higher resolution output \cite{schwab2004ai}.
	\item Trees don't scale well, and highly complex trees can be very difficult to maintain
\end{itemize}

\subsection{Genetic Algorithms}



Pros:
\begin{itemize}
	\item When it may be too computationally expensive to calcuate a decision
	\item To complement other AI techniques
	\item Relatively simple to set up and start getting results, even if you don't know how to solve the problem otherwise
\end{itemize}

\noindent Cons:
\begin{itemize}
	\item Evolution is often time-consuming, taking many generations
	\item Not guaranteed to find an optimal or even satisfactory solution
	\item Don't cope well with components which may not have a concrete design; the addition of features can require a complete redesign of the genetic algorithm
\end{itemize}

\subsection{Other techniques}

\begin{itemize}
	\item Neural networks attempt to model a brain
	\item 
\end{itemize}

An artificial Neural Network (NN) is an electronic simulation based on a simplified human brain. In an NN, knowledge is acquired from the environment through a learning process and the network’s connection strengths are used to store the acquired knowledge [20].

Choosing the variables from the game environment that will be used as inputs is the most labour intensive part of developing an NN [30]. This difficulty is due to the fact that there is wealth of information that can be extracted from the game world and choosing a good combination of relevant variables can be difficult. Also, the number inputs needs to be kept to a minimum to prevent the search space from becoming too large [9]. Therefore, it is a good idea to start with the essential variables and add more if required. Choosing inputs that are poor representations of the game environment is the primary reason for failed applications.NNs are techniques that can be used in a wide variety of applications. Some common uses include memory, pattern recognition, learning and prediction. There are many commercial applications of NNs across various industries, including business, food, financial, medical and health care, science and engineering [24]. Prominent companies that are using NNs include Microsoft, Sharp Corporation, Mars, Intel, John Deere, Mastercard, Fujitsu and Siemens [24]. Some examples of applications that NNs are being used for are predicting sales, handwritten character recognition for PDAs and faxes, odour analysis via electronic nose, stock market forecasting, credit card fraud detection, Pap smear diagnosis, protein analysis for drug development and weather forecasting. This list illustrates the wide variety of applications that can make successful use of NNs, and how their usefulness is only limited to what can be imagined.The computer game industry is no different from the industries mentioned above in terms of the variety of applications of NNs. A few applications are described by LaMothe [26], including environmental scanning and classification, memory and behavioural control. The first application, environmental scanning and classification, involves teaching the NN how to interpret various visual and auditory information from the environment, and to possibly choose a response. The second application, memory, involves allowing the AI to learn a set of responses through experience and then respond with the best approximation in a new situation. Finally, behavioural control relates to the output of the NN controlling the actions of the AI, with the inputs being various game engine variables. Also, the NN can be taught to imitate the human player of the game [31].

Basically, an NN can be used to make decisions or interpret data based on previous input and output it has been given. The input can be seen as various games states, similar to that used by a state machine, and the output could be the action to be performed. The important difference is that the current state doesn’t need to have been hard-coded. Instead, the NN makes the best approximation that it can, based on the states that it already knows about. This means that the NN will choose an action that would have been performed in a similar state.
So far, game developers have been reluctant to allow a game to ship with the learning in NNs and other techniques “switched on”, in case the AI were to learn something stupid [46]. Therefore, the developers that have used NNs in their games have not used them for learning, but rather trained them during development and locked the settings before shipping. Some examples of games that include NNs for various tasks include BattleCruiser: 3000AD, Black \& White, Creatures, Dirt Track Racing and Heavy Gear.In BattleCruiser: 3000AD (BC3K) the AI uses NNs to control the non-player characters as well as to guide negotiations, trading and combat [45]. It uses the development language AILOG (Artificial Intelligence \& Logistics), which was created by the developer of BC3K, Derek Smart, and uses an NN for very basic goal oriented decision making and route finding, with a combination of supervised and unsupervised learning. In Black \& White the player has a creature that learns from the player and other creatures. The creature’s mind includes a combination of symbolic and connectionist representations, with their desires being represented as NNs [15]. Finally, the Creatures series of games makes heavy use of Artificial Life techniques, including heterogeneous NNs, in which the neurons are divided into lobes that have individual sets of parameters. In combination with genetic algorithms, the creatures use the NN to learn behaviour and preferences over time.In short, NNs are techniques that can be used for a wide range of applications in many different environments. Several commercial games have used this technique successfully, with the most recent and prominent being the game Black \& White. This technique’s flexibility means that it has the potential to be applied in a wide range of situations in future games. Therefore, it is likely that NNs will play a bigger role in commercial games in the near future.

\subsection{Applications For Machine Learning}

While it has yet to be utilised prominently in the game industry, machine learning is already used very effectively in many other application areas.

Tom Mitchell outlines many current application areas for machine learning in his 2006 article \emph{The Discipline of Machine Learning} \cite{Mitchell:2006fv}:
\paragraph{Speech recognition:} The commercial systems currently available all use some form of machine learning to train the system. Many systems even use a two-phase learning method: an initial phase for more general learning which is carried out prior to the product being shipped, and a second phase which tunes the system to the customer, and takes place after a customer purchases the product.

\paragraph{Computer vision:} Many of the currently available vision systems utilise machine learning techniques. One of the largest scale vision systems is in use by the US postal service to sort letters with hand-written addresses. According to Mitchell's paper, over 85\% of handwritten mail in the US is now sorted automatically using this system.

\paragraph{Robot control:} A further application for machine learning methods is for robotic control. For example, researchers at Stanford University lead by Andrew Ng have demonstrated a machine learning system capable of advanced helicopter flight \cite{Ng:2004dz, Abbeel07anapplication, Abbeel:fu}. This type of system could be particularly useful in space and planetary exploration in the future.

\paragraph{Accelerating empirical sciences} Machine learning methods are also being employed to aid in scientific discovery, such as gene analysis, in the field of astronomy for sky surveillance and to analyse brain activity. Machine learning methods are particularly useful in data-intensive areas for obvious reasons.

\subsection{Machine Learning In Games}

\begin{itemize}
	\item Commercial Examples
	\item Academic Research
\end{itemize}

One of the few commercial games to employ some more advanced methods of AI to real effect was Lionhead's Black and White. However, despite Black and White's success, no other developers have picked up the mantle.

\section{Conclusion}

I feel that the possibility of utilising academic machine learning techniques into commercial computer games is an often overlooked technique which is not only possible, but feasible with todays technology; something I have proven by showing this on a mobile platform. 

Machine learning in games could have many possible applications. The obvious use is to create much 'smarter' AI-controlled characters which are capable of modifying their behaviour dynamically to adapt to unforeseen scenarios. This could be applied to virtually any game genre, from enemy commanders in a real-time strategy war game, to the AI-controlled players in a football game, to more intelligent enemy characters in a platform game. Some applications are often more pertinent than others; performance is still of paramount importance, and is still an issues; mostly so with console games where the hardware capabilities is strictly limited, and even more so on mobile devices.
	
However, while machine learning may appear to be the answer to all our game-related AI problems, there are a number of possible pitfalls associated with using machine learning techniques.

A WARNING
Learning is not as widely used as you might think, partly due to the relative complexity of learning techniques, but mostly due to the reproducibility and quality control issues it presents.

It is impossible to avoid the AI learning the “wrong” thing. The more flexible your learning is, the less control you have on gameplay.

The normal solution to this problem is to constrain the kinds of things that can be learned in a game. It is sensible to limit a particular learning system to working out places to take cover, for example. This learning system can then be tested by making sure that the cover points it is identifying look right.

Under this modular approach there is nothing to stop several different learning systems from being applied (one for cover points, another to learn accurate targeting, and so on).

OVER-FITTING

A common problem identified in much of the AI learning literature is over-fitting, or over-learning. This means that if a learning AI is exposed to a number of experiences and learns from them, it may learn the response to only those situations. We normally want the learning AI to be able to generalize from the limited number of experiences it has to be able to cope with a wide range of new situations.Different algorithms have different susceptibilities to over-fitting. Neural networks particularly can over-fit during learning if they are wrongly parameterized or if the network is too large for the learning task at hand.

7.1.7 THE BALANCE OF EFFORT

Learning algorithms are attractive because you can do less implementation work. You don’t need to anticipate every eventuality or make the character AI particularly good.

If a problem is very tricky for a human being to solve and implement, then it is likely to be tricky for the computer to learn too.
	
\chapter{Choice of Tools}

Although it may seem a fairly trivial task at first, choosing the right tools to develop my project was not an easy job; I had to make sure that the applications and engines/code libraries that I chose would be suitable for their intended purpose before I started any serious development, as trying to switch mid-way through the development process would undoubtably cause big problems were I to try and migrate the existing code to use other tools/libraries.

\section{Programming Languages \& Game Engines}

The first, and arguably the biggest decision I had to make was which language I was going to develop my project in. My choice of language would not only have a big influence on the performance (and so the scale/features I would develop), but also the hardware that my game would run on.  

I initially planned to develop my project in Java. I felt that Java was an appropriate choice for a couple of reasons: the first and foremost being that I was already comfortable using the language, as it had been taught in several modules I had studied. Secondly, I was attracted to the cross-platform nature of Java; not only could I develop my application to run on the Windows, Mac OS X and Linux platforms, but it could also be embedded in an applet and run in a web-browser. This meant that collecting testing data would be much easier, as I could have the data sent to a central database and processed. In addition, the actual testing process itself would be much easier due to the fact that my product would be available to all desktop users, as opposed to limiting the user base to a single platform.

After some initial research, I found the JMonkey engine (JME); a 3D game engine written in Java with some pretty nice features: a nice IDE based on the popular NetBeans, asset management, integrated GUI elements, applet integration and networking to mention a few. 

Before deciding on Java/JME as my tools of choice, I had a play around with the IDE and did a few tests. The first think I noted about the engine, was that it was overly complex for my needs. This isn't necessarily a fault of JME or Java, but rather of my choice to look at 3D game engines in general. I had no need for the more advanced features such as complex physics, lighting, texturing and materials, animation, cameras etc. These features were completely overkill for my needs; even setting up a basic side-on view took a considerable amount of code, which involved setting up cameras, loading 3D models etc. None of which were really necessary for my project. In addition, I didn't feel that 3D particularly benefitted me from an aesthetic point of view. Additionally, I also had a few issues with a low framerate when adding many objects to the scene. Although this was likely due to errors with my implementation rather than the engine itself, it highlighted to me the problems with developing in Java, and its lack of any memory management - at least from the point of view of the programmer. I have also experienced similar performance issues when using the Swing interface bundled with Java which is notoriously poor from a performance perspective. Due to these points, I decided against trying to use a 3D engine and programming in Java, and instead began to research 2D game engines.

My research eventually led me to Cocos2D: a 2D game engine originally written in Python, which has since been ported to many other languages including C++, Java, Ruby and Objective-C. Using this engine opened up the possibility of developing my game for a mobile platform; something which greatly interests me (although admittedly, the JME does offer Android support).

I felt that Cocos2D was particularly well suited to my project as first and foremost, it is a well-established engine optimised for developing 2D games, with asset loaders, particle systems, and scene management to name but a few useful features. It also comes bundled with two popular physics engines (Box2D and Chipmunk). Another point worth noting is that being a 2D engine, Cocos2D already has a thriving community of developers with experience making 2D games, whereas the JME is intended for 3D games, and as such, there was a lot less documentation for 2D games.

Another benefit to using the Cocos2D engine is that it comes with libraries of code specifically optimised for games. For example, the engine comes with an alternative collection named 'CCArray' which is essentially an optimised version of the default NSArray class. There is also a 'TextureCache' class which 

TextureCache. font rendering, sprite sheets, sound support. Actions for 'automatic' animation (interpolation).

After testing out a few example apps, I decided that Cocos2D would be suitable for my needs. 

I decided to develop my project for the iPhone, so would need to learn Objective-C; a challenge I was willing to accept. 

One of the main benefits to using Objective-C over Java is that it offers memory management, and so affords the developer much more control over their application. Although this may not be so much of an issue with modern desktop systems, the amount of available memory in a mobile environment is still fairly limited. Objective-C is also comparable to C++ in terms of runtime 'speed', due to the fact that Objective-C can be seen as really C (due to the fact that Objective-C is a strict subset of C) so any 'heavy lifting' code can be written in C to squeeze out the maximum performance. Objective-C also allows the developer to write code in C++ (called Objective-C++) if need be; something that would be necessary had I chosen to utilise the Box2D physics engine.

Another benefit to using Objective-C is having the option to use Apple's Xcode as a development environment, as well as the Instruments performance profiling tools to get detailed analysis of the program's runtime performance.

\section{Asset Production}

Looking past the code, I also needed to create a variety of assets for my game: backgrounds, character sprites, menu items, fonts etc. COCOS2D SUPPORTED FILE FORMATS. 

For the graphical assets, I used an image manipulation program called Pixelmator[WEBSITE] which exports to the PNG image format which I would be using for the majority of my game's assets. 

Sprite batch node: In order to utilise a performance optimisation feature of the Cocos2D engine called the sprite batch node I needed to create texture atlass or sprite sheets, which contained the images I would be using in my game. BENEFITS - why not use for backgrounds etc.

For the fonts, I used a program called bmGlyph[WEBSITE], which has been specifically developed for use with game engines that support the FNT file format.

\section{Version Control}

Using some form of version control system is a necessity in modern software development; it provides an easy way for multiple developers to collaborate on a single project (or even a single file) without causing file corruption. It is an invaluable tool for modern software development teams where it is not uncommon for developers to be in different cities or even continents. It also tracks all changes made throughout the development of the project. This is perhaps the most useful function of any version control systems as it not only allows the developers to see exactly what was changed (and by whom) in a project's history, but also means that were a mistake made, the project could easily reverted to an earlier state. Although I'm unlikely to experience any problems with file corruption as I'm the only one who will be contributing to the code-base, using a version control system still provides an incredibly useful way for me to not only track the progress of my project, but also as a way to incrementally back-up my project to ensure that any data loss is recoverable.

There are a variety of different source control systems available, such as CVS Subversion, Mercurial and Git. I decided to use Git as the version control system for my project for a couple of reasons. The first reason being that it is widely supported by many IDEs and other version control management software, with a number of services offering free Git hosting. I also generally prefer Distributed Version Control Systems (DVCS) as opposed to centralised systems for the reason that I like the freedom that DCVS systems give you in terms of being able to work on a project without the need for an internet connection. I also feel that DVCS systems provide more security in that each user has a backup of the entire system. However, this is not necessarily a benefit, as it could take up an inordinate amount of storage space were I working with many binary files (for example Adobe Flash files). In this case, using a DVCS would be unsuitable.

The Git version control system is actually very simple; it tracks all of the files in a project (known as the 'tree'), with each change submitted to the system (called a 'commit') consisting of a snapshot of the state of each file in the tree at that point in time. Specific commits can be 'tagged' to mark their significance; a feature which is normally used to mark specific releases of a project. Part of the reason Git is so efficient is that it uses an encoded string called a SHA hash (essentially a 'fingerprint' of the file) to determine whether or not a file has been changed; checking just requires comparing the local SHA to the remote SHA. 

I will be using a Git server hosted by GitHub to store the source code and documentation for my project. 

\section{Project Management}

To manage the progress of my project, I used a web-based application called ProjectPier which I installed on my web server so could be accessed from anywhere.

Using this system, I split my project into a number of 'task-lists' for each component in my project's development; for example: asset production, game development, AI development, testing, etc. I also added milestones to my project to mark specific targets that I wanted to achieve by certain dates.

Using this system, I could add individual tasks that needed to be completed, deadlines (or 'milestones') to mark specific targets in the development of my project. I could also use the system to create a wiki for my project containing useful information. 



%
% New part
%

\part{Planning}{This section covers the process of designing my system, from the initial concept art and art direction, to the system functionality modelling.}

\section{Analysis:} "a vague understanding of the problem is transferred into a precise description of the tasks that the software system needs to carry out."

\subsubsection{Creation of a functional specification:}
\begin{itemize}
\item Completely defines the tasks to be performed
\item Free from internal contradictions 
\item Can be tested against reality
\end{itemize}

The analysis phase concerns itself wit the description of what needs to be done, not how it should be done.

\section{Design:} the designer must structure the programming tasks into a set of interrelated classes. Each class must be specified precisely, listing both its responsibilities and its relationship to other classes in the system.

\subsubsection{Main goals:}
\begin{itemize}
\item Identify the classes
\item Identify the responsibilities of these classes
\item Identify the relationships among these classes
\end{itemize}

Not necessarily a complete process; not all classes may be discovered until implementation.

\subsubsection{Components:} 
\begin{itemize}
\item A textual description of the classes and their most important responsibilities
\item Diagrams of the relationships among the classes
\item Diagrams of important usage scenarios
\item State diagrams of objects whose behaviour is highly state-dependant
\end{itemize}

\chapter{Initial Artwork}
\chapter{UML/Class Diagrams}
	
\chapter{Social, Legal, Ethical and Professional Issues}
		\section{Intellectual Property}
		\section{Copyright Law}
		\section{The Data Protection Act}
		\section{Research Ethics}



%
% New part
%

\part{Implementation}{This section covers the process of actually implementing my system, problems that I was faced with, and how I solved them.}

\chapter{Game Component}
		
\section{Code Structure/Features}

METHOD PARAMETERS PREFIXED WITH UNDERSCORE

Before I could build any learning into my project, I first had to build the underlying game component which would run it. The game component of my project was XXX. 

As I was using the Cocos2D engine, a lot of the very low-level functionality had been taken care of. For example, the direct OpenGL ES calls needed for loading and binding textures were all contained within the CCSprite class, making adding new textures as easy as initialising a CCSprite class. 

Although the very low-level functionality had been simplified by the engine, 

As I was building my game from the ground up (albeit with the help of the Cocos2D framework), I needed to take care of the fairly low-level features such as loading in the level assets, building the levels; to load in the levels and sprites

\subsection{Singleton/Manager classes} I make a lot of use of the singleton design pattern throughout my project to create various 'manager' classes to deal with the shared data in my system. A singleton class is a special kind of class which only has one instance; calling a singleton instance will always return the one instance of the class, regardless of which class called it. The benefit of using the singleton pattern is that you only need to store the data in one central location, which can then be accessed by any class anywhere in the program; provided that the access to the data is restricted to a single thread at a time (otherwise there would likely be concurrency issues with data corruption). For example, my LemmingManager class as its name suggests manages all of the lemming characters in the game. It hold the only list of the characters, and contains the functionality to add and remove lemmings among other things. Using the singleton 

Restricts the instantiation of a class to one object; useful when exactly one object is needed to coordinate actions across the system. When you want to ensure that no additional instances of the class are created accidentally.

\subsection{Levels} When it came to designing how the levels would work in my game, I wanted something which was very flexible, in that I could easily add extra levels (or modify existing levels) with a minimal amount of work. The way that I decided to do his was to create a number of generic assets which I could re-use in each level. Things such as platforms, water hazards, trapdoors etc. This meant that when I wanted to add an additional level to the game, I could simply specify the positioning of these existing elements, rather than having to create new level assets per level. By using re-usable assets, not only was adding new levels a lot simpler, it also meant that I could drastically cut down on the amount of disk-space that my game needed.

To accompany the assets, I also needed some way to have some way to specify the layout of each level, as well as other level specific data. To accomplish this, I used the natively supported Property List (plist) file format. Plist files are simply XML files of dictionary objects, consisting of a key object (usually a string), and a 'value' object. One of the advantages to using the plist is that it's easily editable in any text editor. It also supports a variety of datatypes, such as strings, numbers (ints, floats, etc) and dates. You can also nest dictionaries inside each other for even more flexibility. 

I used one plist file to specify how many levels there were in my game, along with certain level-specific information such as difficulty, number of tool uses. I then created a plist file for each level, which stored the positions of each piece of terrain, as well as the type of the terrain, and in some cases whether the terrain piece was collide able (this was enabled by default, so was only included if the object was to be removed from collision detection). My game then loaded in the list of the levels, and each individual level plist. 

Initially, I planned to have levels of varying difficulty, with an option on the new game screen to allow the user to select a difficulty. However, I found it difficult to determine what an 'easy' level should be for example, and so the final game just assumes all of the levels are the same difficulty.

\subsection{Basic Functionality} Agent behaviour (finite state machine), winning the game

\subsection{Constants} I created a file to contain the constants used in my program (for example the framerate, the default font filename and the various rewards associated with reinforcement learning). Moving all of these variables into a single separate file means that Each constant in this file follows the standard naming convention for constants (i.e. starting with a "k"). This is a legacy feature from the pre-Mac OS X days, possibly back when the Mac OS was written mostly in Pascal. It's interesting that this convention is kept up, given that it doesn't follow the standard C convention of defining constants in capital letters. 

\subsection{Datatypes} Similar to the constants file. Defines all of the datatypes/enums used in my game.

\subsection{Utility Methods} I created a Utils class with some useful static-access 'utility' methods which I was likely to want to use a lot in my game; functions such as random number generation, enumeration-to-string conversions, and timestamp generation. It made sense to create a separate class with all of these types of methods because it not only cut down on the amount of code that I had to write, but also meant that were I to find a bug in my random number generation code for example, I would only need to fix the bug in one place, rather than having to go and search every source file for occurrences of the broken code. 
		
\subsection{Deviance From The Design} Overall, I think I adhered to my original designs pretty 

\subsection{Performance Optimisations} As already mentioned, Cocos2D offers a variety of performance optimisations specifically tailored for developing games. One such optimisation is the 'CCSpriteBatchNode'. It is not uncommon to experience poor performance when there are a large number of objects onscreen at once. 

For each sprite, OpenGL ES must first bind to the texture, and then render the sprite. As more and more sprites are added to the screen, it isn't difficult to see that the number of calls to OpenGL will steeply increase too, with every call costing a few CPU cycles. It is basic common sense to see that the game will run faster the fewer calls to OpenGL are made. 

The CCSpriteBatchNode is a Cocos2D class which has been created to help with this problem. The CCSpriteBatchNode works by taking a single texture which contains all of the textures needed by the current scene (called a texture atlas, SEE DIAGRAM), and sends all of the images for rendering to OpenGL at once, rather than individually. This essentially reduces the number of bind calls needed from O(n) to O(1) (in Big-O notation). Using texture atlases also cuts down on the amount of memory needed to store each individual image. In some version of the iOS, textures were required to be stored in sizes of the poet of two (64, 128, 512 etc.). This meant that the textures would often have a lot of unnecessary white space around the image (SEE DIAGRAM), which obviously takes up extra memory; something which even with todays devices is not a commodity. An additional optimisation which can be made regarding the textures is to use the 	compressed PVR CCZ file format. In addition to saving disk space in terms of the actual size of the image, the PVR CCZ format is supported natively by the iPhone's GPU, and can be loaded directly onto the GPU without the need for conversion.

Aside from the performance optimisations offered by Cocos2D, I also did a few other things to ensure that my program ran as efficiently as possible.

\subsection{Random number generation} Although it may not seem like a likely source for a performance bottleneck, the random number generator (RNG) I chose to use could have a noticeable impact on performance. Based on some very quick tests carried out by a poster on the official Cocos2D forums (http://www.cocos2d-iphone.org/forum/topic/11290), I was surprised to see the difference in speed between different RNGs. SEE TABLE. Typically, the RNG I had chosen to use (arc4Rand) was by far the slowest method tested. Based on the tests carried out, I instead decided to use the CCRANDOM\_0\_1 function, which was shown to be 5 times quicker. Although the tests carried out on the forum were tested by performing 5 million iterations of the algorithm, far more than I would ever need to use, every performance enhancement helps.
		
\subsection{Problems Faced} General problems learning the language, differences between languages learnt already. Can only add objects to an array - need to use [NSNull null]

An array can contain only objects (same as Java, but Java hides this from the user by wrapping core types in objects). Similar fix in Objective-C, but have to do it manually (e.g. [NSNumber numberWithInt:1]).

\subsection{Memory management} Something which I found particularly tricky about using Objective-C over a language like Java was the memory management. It isn't until you start to learn another language that you realise how much the Java runtime shields the programmer from the low-level bugs associated with memory management thanks to the garbage collector. In Java, memory is reserved for an object simply by using the 'new' keyword, and memory is automatically released when that object goes out of scope and is no longer accessible. This could be by the programmer setting the object to null. At this point, the garbage collector reclaims the memory.

On the other hand, languages such as Objective-C  require manual memory management, meaning that every object created needs to be manually released by the programmer when finished. 

The big problem with memory leaks is that they can often go undetected, only to cause the program to unexpectedly crash later on down the line. For this reason, they can be notoriously difficult to track down. 

I discovered a particularly nasty bug in my game's update method for example, which is called every frame, whereby I was allocating an array to hold all of the game objects, but failing to release the memory at the end of the method. 

I discovered that I had made this very error in my game's main update method (which is called 60 times per second). Using Instruments, I found that this relatively minor error was losing a lot of memory, and causing the game to drop 15 frames per second after about 15 minutes. Although this may not seem like much, it had a knock-on effect on my game timer, which determined the game time by counting each frame. Therefore, losing 15fps meant that I was essentially losing one second in every five. 

Carrying out this project really taught me the importance of keeping a close eye on what memory you're allocating and where you're releasing it. What may at first seem like a negligible memory leak can build up over time, and combined with other similarly small memory leaks, can cause big problems with your programs performance. I would even go so far as to say that these smaller memory leaks are even more troublesome than the bigger ones, as they are much harder to track down, and are often more widely spread, and therefore more difficult to fix.

Xcode comes with a very useful set of performance analysis tools called 'Instruments' which can be used to establish any memory-related or runtime issues, such as memory leaks etc.

Another problem I frequently came across which couldn't happen in Java was that of initialised variables. Whereas in Java, whenever you declare a new variable, it's automatically initialised with whatever the default value may be, you get no such privilege in Objective-C. When you declare a variable in Objective-C, it stays uninitialised until you specifically set it. This caused me the biggest problems with arrays; trying to access an uninitialised array wouldn't cause a compiler error, but would cause a crash at runtime.


\section{OpenGL ES} The Open Graphics Library for Embedded Systems (OpenGL ES) is a subset of the OpenGL 3D graphics API which targets embedded systems such as the Nintendo 3DS, the Sony Playstation 3, and devices running Apple's iOS and Google's Android operating systems.

\section{Cocos2D} The Cocos2D framework uses a hierarchical structure to manage the many components which make up a typical game. It uses the analogy of film production to make this easy to understand. At the highest level is the CCDirector class which, as one would expect, controls the running of each individual level in the game (known as CCScenes). Each scene consists of one or more  layers (CCLayers) which contain the actual game objects (for example the background image, the level terrain and the game characters). The game objects could take the form of CCSprites (at it's most basic terms, a sprite is an image displayed onscreen)

The Cocos2D framework is loaded via the AppDelegate singleton class, a class which deals with the basic running of the application; what should happen when launching the app, when the user closes the app, when the app is sent to the background etc.

In computer programming, a constant is an identifier whose associated value cannot typically be altered by the program during its execution.

\chapter{AI Component}

\section{Code Structure/Features}

Asynchronous approach of updating the Q-values; updating a single pair at the end of every episode (synchronous - update all episodes once)

\section{Deviance From The Design}
	
\section{Performance Optimisations}

\section{Problems Faced}
	
	
	
%
% New part
%

\part{Testing \& Evaluation}{This section presents the usability and learning testing I performed, the results that were obtained, and an analysis of the product's performance.}

\chapter{Testing \& Results}

Show the results of usability testing (if any) and the learning results. Compare with the different types and no learning, show graphs to compare. Discuss the effectiveness, advantages/disadvantages to the learning algorithms used.

\section{Usability Testing}

\section{AI Testing}

\subsection{Analysis of Decision Tree Learning}
\subsection{Analysis of Reinforcement Learning}



%
% New part
%

\chapter{Evaluation}

A very competent evaluation of the whole project (with hindsight). Weak performance in the evaluation of product and process will result in a grade no higher than a B.

\section{Time Management}

Initial plan was to use a waterfall development approach (discuss). Eventually took a slightly more incremental approach: developed game and AI together towards the end, some asset creation throughout. Discuss why, which would have been better - for one person rather than a team. As I was unfamiliar with the language and the engine, it was difficult to decide how best to approach the design/implementation. (see \url{http://en.wikipedia.org/wiki/Software_development_methodology})
		
- Waterfall: a linear framework
- Incremental: a combined linear-iterative framework
- Spiral: a combined linear-iterative framework
		
The biggest miscalculation with regards to my time-management and project schedule was that I failed to recognise just how much groundwork was needed to build the game component. Though I by no means expected the game development to be a trivial task, I wasn't prepared for the amount of work that was needed. This was exacerbated by the fact that I was also still getting to grips with Objective-C, and so my productivity wasn't as it could have been. However, although I slipped by a couple of weeks during the game development stage, I finished the AI component of the game under-schedule. Looking back at the volume of code that was needed to get the game running, it seems fairly that I should have at least allocated an equal amount of time to both the game and AI components. As it happens, I still finished my project to schedule with some time to spare.

\section{Achievements}

\section{Evaluation of reinforcement learning}

+ it can handle uncertain and noisy domains
+ it can interface with an external world more easily than other methods
- slow learning rate as they rely on backwards propagation of rewards. The problem solver must pass through early parts of the problem space many times before any rewards at all reach those locales.


\section{Areas For Enhancement}

Although I'm very pleased with the outcome of my project, given extra development time, there are a number of areas which I feel I would like to extend/build upon.

The most obvious enhancement that I could make would be to extend the AI system to incorporate more learning types. Were I to do this, the next learning algorithm that I would like to try would be a genetic algorithm, as I think that it lends itself quite well to the context of my game. I could slightly alter the spawning of the lemmings so that they all spawn in waves, a certain number at a time - say 100. Then, at the end each wave, I could evaluate each lemming's performance, choosing to breed those with the most effective policy.

Another enhancement that I think it would be interesting to make would be to allow for multiple learning types to be used in a single game, with separate agents able to use different algorithms. It would be interesting to compare the performance of the two. Following on from this idea, I also think it could be interesting to mix different methods of learning together in one lemming…

Something else which I'd like to implement would be some way for the agents to learn from each other - perhaps by watching other agents. This obviously wouldn't work with the shared knowledge base, but would be interesting to see the improvement of learning this way over the original.

Another enhancement that I could make to the game would be to extend the system to a 3D environment. This would obviously complicate things, but.

- -Implement additional learning types-
- -Mixed learning types between agents-
- Use a combination of types
- Remove the random learning episodes
- Communication between agents (shared knowledge base?)
- Extend to a 3D environment
- Level editor
- "Start with a large correction factor (learning rate), and slowly decrease the parameter, which gives the algorithm a rough approximation during the early stages and allows fine tuning later in learning."
- Test alternative reinforcement methods (bucket brigade, temporal difference)
- Negatives of using reinforcement learning: it doesn't generalise beyond specific states. Possible fix: use genetic algorithms - "the expected reward on production rules serves as their fitness, with more fit rules being selected for reproduction. The new rules are then evaluated by running them in an environment, with the best of them being selected to produce another generation, and so on. Such classifier systems are often used in conjunction with the bucket brigade algorithm for determining expected rewards." 

\section{Overall Success}
- Applications For Learning in games (possibly pre-release learning)
- Performance on mobile device



%
% New part 
%

\part{Appendices}{}

	\appendix
	
	\chapter{Code Listings}

	\chapter{Project Log}
	
A day-by-day work diary kept by you as the project is carried out. A log which has evidently been manufactured to be handed in rather than having been properly kept will result in a lower grade.

	\chapter{Git Log}


	
%
% References
%

\singlespace

\newpage
\addcontentsline{toc}{part}{Bibliography}
\bibliographystyle{abbrv}
\bibliography{Cogito}

\end{document}
