\documentclass[a4paper,oneside]{report}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr} 
\usepackage[pdftex]{graphicx}\usepackage{listings}      
\usepackage{pdfpages}
\usepackage{setspace}
\usepackage{url}

\makeatletter

\begin{document}

\setcounter{secnumdepth}{-1} 
\onehalfspace
\oddsidemargin 1in 

% add horizontal lines
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\HRuleLight}{\rule{\linewidth}{0.1mm}}

% custom part page
\def\part#1#2
{
	\par\break
  	\addcontentsline{toc}{part}{#1}
	\noindent
	\null	
	\HRuleLight\\[0.0cm]
	\vspace{20pt}	 
	\begin{flushright} 		
  	{\Huge \bfseries \noindent #1}\\
  	\vspace{30pt} 
	\begin{minipage}{0.85\textwidth}
		\begin{flushright}
		{\large \noindent #2}
		\end{flushright}
	\end{minipage}\\[0.75cm] 
	\end{flushright} 		
	\thispagestyle{empty}
	\break
}

% chapter header
\renewcommand{\@makechapterhead}[1]
{\vspace*{50\p@}{
	\parindent \z@ \raggedright \normalfont	%\huge \bfseries \thechapter. #1
	\huge \bfseries #1	\vspace{20pt}}}

%
% Cover page
%
\begin{titlepage}
\begin{center}

\includegraphics[width=120mm]{sources/images/cogito_logo_main.png}

\HRuleLight\\[0.5cm]

\begin{minipage}{0.45\textwidth}
	\begin{flushleft}\large
		\emph{Author:}\\
			\textbf{Thomas \textsc{Taylor}}\\[0.27cm]
			Computer Science (Games)
			Student Number: 08813043
	\end{flushleft}
\end{minipage}
\begin{minipage}{0.43\textwidth}
	\begin{flushright} \large
		\emph{Supervisor:} \\
		\textbf{Graham \textsc{Winstanley}}\\[0.25cm]
		\emph{Second Reader:}\\
		\textbf{Saeed \textsc{Malekshahi}}
	\end{flushright}
\end{minipage}\\[0.75cm] 

\HRuleLight\\[0.2cm]

\large School of Computing, Engineering and Mathematics\\ \textbf{University of Brighton}

\vfill
\huge Project Documentation\\
\large April, 2012\\

\end{center}
\end{titlepage}

%
% Table of contents
%
{
	\renewcommand\thepage{}
	\setcounter{tocdepth}{3}
	\tableofcontents
	\clearpage
}

% reset page count
\setcounter{page}{1}

%
% Start of content
%

\paragraph{Project proposal}

\begin{quotation}
\emph{The main aim of my project is to develop an AI system that is capable of employing machine learning techniques in order for a number of AI-controlled agents to safely navigate a game environment.
The agents will be able to develop a knowledge-base dynamically based on the observations made whilst navigating the environment and apply this in order to traverse the world safely.
The main deliverable will be a software-based demonstration of the system.}
\end{quotation}

\chapter{Introduction}

The main aim of my project is to develop an AI system that is capable of employing machine learning techniques in order for a number of AI-controlled agents to safely navigate a game environment. The agents will be able to develop a knowledge-base dynamically based on the observations made whilst navigating the environment and apply this in order to traverse the world safely. The main deliverable will be a software-based demonstration of the system.

For the main deliverable of my project, I have chosen to create a game similar to the popular puzzle game Lemmings, and replace the human control element with my AI system.

\section{Background}

The classic Commodore Amiga game 'Lemmings' was the main inspiration for my project.

Originally released in 1991 for the PC and Commodore Amiga, the game has a very simple premise: to guide a group of computer-controlled ’lemmings’ across a level from the entrance-point to the exit. The lemmings themselves, although computer controlled, have no AI to speak of, and merely walk in one direction until they reach an immovable object (such as a wall) or a trap (water, spikes, big drops etc.), the latter resulting in the unfortunate demise of the lemming. Each level requires that a certain number of lemmings reach the exit in order for the player to progress. To complete a level, the player is given a number of tools to help them such as umbrellas to avoid big falls, girders to cross pits etc.
		
\section{The Project in Context}
	
Looking back on my project, I feel that it actually covers the majority of modules that I have studied during my course. There were obviously a great deal of skills learnt in the CI213 (Intelligent Systems) and CI342 (Advanced AI) modules which I could directly apply to my project such as pathfinding and search algorithms like A*, MORE STUFF. 

There was also obviously a lot of useful knowledge gained in CI224 (Games Development) which I used throughout my project. In particular, I learnt a lot about memory management in this module which I found very useful as I was using Objective-C, which doesn't offer automatic garbage collection like Java for example. I was also introduced to source control systems in CI224, which has proven to be invaluable knowledge to have.

I also used my knowledge of UML and software design learnt in CI228 (Object-Oriented Software Design and Specification) and CI231 (Formal Underpinnings and Specification) to create designs of my system. Although I didn't actually use the formal specification language learnt in my system specification, I still applied the concepts learnt to my designs.

As I was using Objective-C, I relied heavily on the concepts I had learnt in CI101 (Introduction to Programming), CI228 and CI346 (Programming, Concurrency and Client-Server Computing) to design my system using 'good' OO practices wherever possible. Principles such as inheritance and design patterns like the singleton. I also applied the knowledge learnt this year in CI346 to overcome concurrency issues I had with different threads trying to access shared data simultaneously.

I learnt a great deal about data access and performance optimisation in the CI312 (Computer Graphics Algorithms) module. It really helped me too look at my code at a lower level, and analyse the best and most efficient ways to access and manipulate data. This is something which is of paramount importance, especially in games, as you will likely be carrying out certain functions up to 60 times every second, so any inefficient code would cause big problems in performance. 

%
% Section Page
%
\part{Research}{This section documents the research that I carried out prior to designing my system, and how it influenced my final designs.}
%\chapter{Research}

\chapter{Artificial Intelligence}

(how research has influenced your decisions - there must be evidence of appropriate research)

- Intro to AI, differences between academic AI and game AI

"Learning is the improvement in performance in some environment through the acquisition of knowledge resulting from experience in that environment." - Elements of Machine Learning - Pat Langley

"Learning involves improvement in performance." - Elements of Machine Learning - Pat Langley

"The ability to learn is one of the central features of intelligence" - Elements of Machine Learning - Pat Langley

The motivation for my project came from the fact that most modern games still use very rudimentary techniques when it comes to the artificial intelligence.

Although the reasons for this are very understandable, we reached the so-called pinnacle of graphics development 


\section{AI In Games}
- Smoke and mirrors
- scripting: why use it
	- speed
	- predictability (testing) 
- examples of machine learning in commercial games (Black \& White)

\section{Machine Learning}

- What is it
- types of learning: supervised/unsupervised, online/offline
"in some cases a tutor or domain expert gives the learner direct feedback about the appropriateness of its performance."
"For classification problems, the supervised task assumes that each training instance includes an attribute that specifies the class of that instance, and the goal is to induce a concept description that accurately predicts this attribute. There is less agreement on the goal of unsupervised learning, but one can define analogous prediction tasks over the entire set of attributes. In problem solving tasks, supervised learning occurs when a tutor suggests the correct step at each point in the search or reasoning process; systems that operate on such feedback are sometimes referred to as learning apprentices. However, most work on learning in problem solving has dealt with unsupervised tasks, in which the agent must distinguish desirable actions from undesirable ones on its own." - Elements of Machine Learning - Pat Langley
- what is it used for 

"A computer is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E." - Tom M. Mitchell.

Algorithms that allow computers to evolve behaviours based on empirical data.

Machine learning focuses on prediction based on known properties learning from training data.

Supervised: Machine is given sets of training examples - an input object and an output which are then used to predict an outcome.

Unsupervised: Trying to find hidden structure in unlabelled data - no error or reward signal to evaluate a potential solution.

Semi-supervised: Makes use of both labelled and unlabelled data. For example, 2+ learners are trained on the same data, but each learning uses a different set of features for each example. 

\subsection{Reinforcement Learning}

For most applications, reinforcement learning falls into 3 categories: Dynamic Programming, Monte Carlo, and Temporal Difference.

Dynamic Programming
Dynamic programming is used to solve learning problems where state transition probabilities are known beforehand. The state transition probability function, p(s,a,s`) is the probability of transitioning from state s to state s` when the agent takes action a. The goal of the learning agent is to learn to approximate the reward function V(s,a), which gives the expected reward of taking action a in state s. The agent approximates the reward function by testing out different actions in different states, each time receiving a reward r. For taking action a in state s, the agent updates V(s,a) with the following function:

V(s,a) <= r + gamma * sum(p(s, a, s`) * V(s')) (Eq. 1)

Where gamma is a parameter known as the discount rate, the summation sums over all states s` and V(s`) represents the maximum V(s`,a) for all actions a. This equation is known as the Bellman equation.

Monte Carlo and Temporal Difference
Monte Carlo and Temporal Difference methods are used to solve problems where p(s,a,s`) is not known. Temporal Difference solves the learning equation by using the expected reward of the next state it encounters after taking an action a in state s. The full equation is:

V(s,a) <= V(s,a) + alpha * ( r + gamma * V(s`) - V(s,a) ) (Eq. 2)

Where alpha is a parameter known as the learning rate. Monte Carlo is similar to Temporal Difference but keeps track of all rewards encountered until an episode terminates in addition to the one state look-ahead performed by temporal difference methods.

As far as the original poster's question goes: reinforcement learning is used because in your problem, there will be many actions that have the possibility to succeed in any given state. Searching for a possible action will not necessarily retrieve the optimal action. That is where learning comes in. For wall following you will probably have a continuous state space and continuous action space, which will considerably complicate the problem.

Regarding other posts in this thread: remember that you are not required to post responses to questions in the forums. If you only have a very vague and shaky understanding of a topic, it is best to keep that vague and shaky answer to yourself. People expect accuracy when they ask questions on the forums, and the goal of gamedev is not to see who can post the most. 

Reinforcement learning is not learning by example, but from rewards.
 
The aim of reinforcement learning is to maximise the expected reward over time. 

Trade-off of long-term and short-term rewards. 

"One approach to the acquisition of problem-solving knowledge, reinforcement learning, focuses on preference knowledge for the selection of operators. Because this class of methods does not require information about operator effects, it is commonly applied to learning from sequences of external actions. However, many of the same issues arise in this context as with internal problem solving, and we will not typically distinguish between them.

In addition to a set of (possibly opaque) operators, a reinforcement learning system is provided with some reward function that evaluates states. Intuitively, states with higher rewards are more desirable than ones with low or negative rewards, and the agent aims to approach the former and avoid the latter. Learning involves the acquisition of a control strategy that leads to states with high rewards, based on experience with the rewards produced by observed operator sequences.

The Simplest approach to reinforcement learning stores knowledge in a table. Each entry describes a possible state-action pair, along with the expected reward that results from applying the action to that state."

"Typically, the problem solver uses this table to direct forward-chaining search. This involves finding the table entry for the current state, selecting the action that gives the highest expected score, and applying this action to produce a new state. The problem solver continues this cycle, producing a forward-chaining greedy search through the state space, until it reaches the desired state or some other halting condition."

How an agent ought to take actions in an environment so as to maximise the end reward. Needs no knowledge o the environment. Correct input/output pairs are never presented, not sub-optimal actions explicitly corrected. Finding a balance between exploration of need territory and exploitation of existing knowledge. Reinforcement learning is well suited to problems which include long-term vs. short-term trade-off. 

Possible applications for reinforcement learning:
-	Robot control
-	Elevator scheduling
-	Backgammon
-	Checkers

"Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs."

"When the agent's performance is compared to that of an agent which acts optimally from the beginning, the difference in performance gives rise to the notion of regret. Note that in order to act near optimally, the agent must reason about the long term consequences of its actions: In order to maximize my future income I better go to school now, although the immediate monetary reward associated with this might be negative.

Thus, reinforcement learning is particularly well suited to problems which include a long-term versus short-term reward trade-off."

1. Set of environment states
2. Set of actions
3. Rules of transitioning between states
4. Rules to determine the immediate reward
5. Rules to describe what the agent observes

An agent interacts with the environment in discrete time-steps. At each step, the agent receives an observation, and chooses an action from those available. The environment is updated, and the reward is calculated.

"Two components make reinforcement learning powerful: 
-	The use of samples to optimise performance 
-	The use of function approximation to deal with large environments

The only way to collect information about the environment is by interacting with it.

(could be considered a genuine learning problem)"

\subsubsection{Markovian Decision Processes:}

\subsubsection{Algorithms:}

- Policy iteration
Heuristic algorithms act almost as an approximation; don't guarantee to find the optimal solution. Deterministic algorithms on the other hand perform predictably, giving the same result each time it's used. 
- Monte Carlo
Mimics policy iteration. 

- Temporal difference
- Bucket brigade
- Q-learning

"Reinforcement learning alters the predicted rewards stored int the state-action table on the basis of experience. The entries typically start with either random values, or equal ones, but the score for a given state-action pair <s,a> changes whenever the problem solver applies action a in state s. This produces a new state and a resulting reward, which the learner uses to revise the existing entry."

"This strategy, known as Q-learning, computes the sum of the immediate reward and the discounted maximum reward expected for the following state, then subtracts the current entry for the state-action pair. It then multiplies the difference by the correction rate and modifies the entry in question by the resulting product. The discount factor specifies the the importance of immediate versus delayed gratification, whereas the correction factor determines the rate at which the learner revises table entries."

"Given enough training cases, this learning scheme converges on the expression Q(s,a) = r(s,a) + discount factor(U(s prime)) as the entry for each state-action pair <s,a> in the table."

"Thus, after sufficient experience, the problem solver can move toward the most desirable state from any location with little to no search"

\subsection{Decision Tree}

\subsection{Genetic Algorithms}

\subsection{Machine Learning/Applications}

\subsection{Why Use Machine Learning In Games}

I feel that the possibility of utilising academic machine learning techniques into commercial computer games is an often overlooked technique which is not only possible, but feasible with todays technology; something I have proven by showing this on a mobile platform. 

Machine learning in games could have many possible applications. The obvious use is to create much 'smarter' AI-controlled characters which are capable of modifying their behaviour dynamically to adapt to unforeseen scenarios. This could be applied to virtually any game genre, from enemy commanders in a real-time strategy war game, to the AI-controlled players in a football game, to more intelligent enemy characters in a platform game. Some applications are often more pertinent than others; performance is still of paramount importance, and is still an issues; mostly so with console games where the hardware capabilities is strictly limited, and even more so on mobile devices.
	
\chapter{Choice of Tools}

Although it may seem a fairly trivial task at first, choosing the right tools to develop my project was not an easy job; I had to make sure that the applications and engines/code libraries that I chose would be suitable for their intended purpose before I started any serious development, as trying to switch mid-way through the development process would undoubtably cause big problems were I to try and migrate the existing code to use other tools/libraries.

\section{Programming Languages \& Game Engines}

The first, and arguably the biggest decision I had to make was which language I was going to develop my project in. My choice of language would not only have a big influence on the performance (and so the scale/features I would develop), but also the hardware that my game would run on.  

I initially planned to develop my project in Java. I felt that Java was an appropriate choice for a couple of reasons: the first and foremost being that I was already comfortable using the language, as it had been taught in several modules I had studied. Secondly, I was attracted to the cross-platform nature of Java; not only could I develop my application to run on the Windows, Mac OS X and Linux platforms, but it could also be embedded in an applet and run in a web-browser. This meant that collecting testing data would be much easier, as I could have the data sent to a central database and processed. In addition, the actual testing process itself would be much easier due to the fact that my product would be available to all desktop users, as opposed to limiting the user base to a single platform.

After some initial research, I found the JMonkey engine (JME); a 3D game engine written in Java with some pretty nice features: a nice IDE based on the popular NetBeans, asset management, integrated GUI elements, applet integration and networking to mention a few. 

Before deciding on Java/JME as my tools of choice, I had a play around with the IDE and did a few tests. The first think I noted about the engine, was that it was overly complex for my needs. This isn't necessarily a fault of JME or Java, but rather of my choice to look at 3D game engines in general. I had no need for the more advanced features such as complex physics, lighting, texturing and materials, animation, cameras etc. These features were completely overkill for my needs; even setting up a basic side-on view took a considerable amount of code, which involved setting up cameras, loading 3D models etc. None of which were really necessary for my project. In addition, I didn't feel that 3D particularly benefitted me from an aesthetic point of view. Additionally, I also had a few issues with a low framerate when adding many objects to the scene. Although this was likely due to errors with my implementation rather than the engine itself, it highlighted to me the problems with developing in Java, and its lack of any memory management - at least from the point of view of the programmer. I have also experienced similar performance issues when using the Swing interface bundled with Java which is notoriously poor from a performance perspective. Due to these points, I decided against trying to use a 3D engine and programming in Java, and instead began to research 2D game engines.

My research eventually led me to Cocos2D: a 2D game engine originally written in Python, which has since been ported to many other languages including C++, Java, Ruby and Objective-C. Using this engine opened up the possibility of developing my game for a mobile platform; something which greatly interests me (although admittedly, the JME does offer Android support).

I felt that Cocos2D was particularly well suited to my project as first and foremost, it is a well-established engine optimised for developing 2D games, with asset loaders, particle systems, and scene management to name but a few useful features. It also comes bundled with two popular physics engines (Box2D and Chipmunk). Another point worth noting is that being a 2D engine, Cocos2D already has a thriving community of developers with experience making 2D games, whereas the JME is intended for 3D games, and as such, there was a lot less documentation for 2D games.

Another benefit to using the Cocos2D engine is that it comes with libraries of code specifically optimised for games. For example, the engine comes with an alternative collection named 'CCArray' which is essentially an optimised version of the default NSArray class. There is also a 'TextureCache' class which 

TextureCache. font rendering, sprite sheets, sound support. Actions for 'automatic' animation (interpolation).

After testing out a few example apps, I decided that Cocos2D would be suitable for my needs. 

I decided to develop my project for the iPhone, so would need to learn Objective-C; a challenge I was willing to accept. 

One of the main benefits to using Objective-C over Java is that it offers memory management, and so affords the developer much more control over their application. Although this may not be so much of an issue with modern desktop systems, the amount of available memory in a mobile environment is still fairly limited. Objective-C is also comparable to C++ in terms of runtime 'speed', due to the fact that Objective-C can be seen as really C (due to the fact that Objective-C is a strict subset of C) so any 'heavy lifting' code can be written in C to squeeze out the maximum performance. Objective-C also allows the developer to write code in C++ (called Objective-C++) if need be; something that would be necessary had I chosen to utilise the Box2D physics engine.

Another benefit to using Objective-C is having the option to use Apple's Xcode as a development environment, as well as the Instruments performance profiling tools to get detailed analysis of the program's runtime performance.

\section{Asset Production}

Looking past the code, I also needed to create a variety of assets for my game: backgrounds, character sprites, menu items, fonts etc. COCOS2D SUPPORTED FILE FORMATS. 

For the graphical assets, I used an image manipulation program called Pixelmator[WEBSITE] which exports to the PNG image format which I would be using for the majority of my game's assets. 

Sprite batch node: In order to utilise a performance optimisation feature of the Cocos2D engine called the sprite batch node I needed to create texture atlass or sprite sheets, which contained the images I would be using in my game. BENEFITS - why not use for backgrounds etc.

For the fonts, I used a program called bmGlyph[WEBSITE], which has been specifically developed for use with game engines that support the FNT file format.

\section{Version Control}

Using some form of version control system is a necessity in modern software development; it provides an easy way for multiple developers to collaborate on a single project (or even a single file) without causing file corruption. It is an invaluable tool for modern software development teams where it is not uncommon for developers to be in different cities or even continents. It also tracks all changes made throughout the development of the project. This is perhaps the most useful function of any version control systems as it not only allows the developers to see exactly what was changed (and by whom) in a project's history, but also means that were a mistake made, the project could easily reverted to an earlier state. Although I'm unlikely to experience any problems with file corruption as I'm the only one who will be contributing to the code-base, using a version control system still provides an incredibly useful way for me to not only track the progress of my project, but also as a way to incrementally back-up my project to ensure that any data loss is recoverable.

There are a variety of different source control systems available, such as CVS Subversion, Mercurial and Git. I decided to use Git as the version control system for my project for a couple of reasons. The first reason being that it is widely supported by many IDEs and other version control management software, with a number of services offering free Git hosting. I also generally prefer Distributed Version Control Systems (DVCS) as opposed to centralised systems for the reason that I like the freedom that DCVS systems give you in terms of being able to work on a project without the need for an internet connection. I also feel that DVCS systems provide more security in that each user has a backup of the entire system. However, this is not necessarily a benefit, as it could take up an inordinate amount of storage space were I working with many binary files (for example Adobe Flash files). In this case, using a DVCS would be unsuitable.

The Git version control system is actually very simple; it tracks all of the files in a project (known as the 'tree'), with each change submitted to the system (called a 'commit') consisting of a snapshot of the state of each file in the tree at that point in time. Specific commits can be 'tagged' to mark their significance; a feature which is normally used to mark specific releases of a project. Part of the reason Git is so efficient is that it uses an encoded string called a SHA hash (essentially a 'fingerprint' of the file) to determine whether or not a file has been changed; checking just requires comparing the local SHA to the remote SHA. 

I will be using a Git server hosted by GitHub to store the source code and documentation for my project. 

\section{Project Management}

To manage the progress of my project, I used a web-based application called ProjectPier which I installed on my web server so could be accessed from anywhere.

Using this system, I split my project into a number of 'task-lists' for each component in my project's development; for example: asset production, game development, AI development, testing, etc. I also added milestones to my project to mark specific targets that I wanted to achieve by certain dates.

Using this system, I could add individual tasks that needed to be completed, deadlines (or 'milestones') to mark specific targets in the development of my project. I could also use the system to create a wiki for my project containing useful information. 

%
% Section Page
%
\part{Planning}{This section covers the process of designing my system, from the initial concept art and art direction, to the system functionality modelling.}

\section{Analysis:} "a vague understanding of the problem is transferred into a precise description of the tasks that the software system needs to carry out."

\subsubsection{Creation of a functional specification:}
\begin{itemize}
\item Completely defines the tasks to be performed
\item Free from internal contradictions 
\item Can be tested against reality
\end{itemize}

The analysis phase concerns itself wit the description of what needs to be done, not how it should be done.

\section{Design:} the designer must structure the programming tasks into a set of interrelated classes. Each class must be specified precisely, listing both its responsibilities and its relationship to other classes in the system.

\subsubsection{Main goals:}
\begin{itemize}
\item Identify the classes
\item Identify the responsibilities of these classes
\item Identify the relationships among these classes
\end{itemize}

Not necessarily a complete process; not all classes may be discovered until implementation.

\subsubsection{Components:} 
\begin{itemize}
\item A textual description of the classes and their most important responsibilities
\item Diagrams of the relationships among the classes
\item Diagrams of important usage scenarios
\item State diagrams of objects whose behaviour is highly state-dependant
\end{itemize}

\chapter{Initial Artwork}
\chapter{UML/Class Diagrams}
	
\chapter{Social, Legal, Ethical and Professional Issues}
		\section{Intellectual Property}
		\section{Copyright Law}
		\section{The Data Protection Act}
		\section{Research Ethics}

%
% Section Page
%
\part{Implementation}{This section covers the process of actually implementing my system, problems that I was faced with, and how I solved them.}

\chapter{Game Component}
		
\section{Code Structure/Features}

METHOD PARAMETERS PREFIXED WITH UNDERSCORE

Before I could build any learning into my project, I first had to build the underlying game component which would run it. The game component of my project was XXX. 

As I was using the Cocos2D engine, a lot of the very low-level functionality had been taken care of. For example, the direct OpenGL ES calls needed for loading and binding textures were all contained within the CCSprite class, making adding new textures as easy as initialising a CCSprite class. 

Although the very low-level functionality had been simplified by the engine, 

As I was building my game from the ground up (albeit with the help of the Cocos2D framework), I needed to take care of the fairly low-level features such as loading in the level assets, building the levels; to load in the levels and sprites

\subsection{Singleton/Manager classes} I make a lot of use of the singleton design pattern throughout my project to create various 'manager' classes to deal with the shared data in my system. A singleton class is a special kind of class which only has one instance; calling a singleton instance will always return the one instance of the class, regardless of which class called it. The benefit of using the singleton pattern is that you only need to store the data in one central location, which can then be accessed by any class anywhere in the program; provided that the access to the data is restricted to a single thread at a time (otherwise there would likely be concurrency issues with data corruption). For example, my LemmingManager class as its name suggests manages all of the lemming characters in the game. It hold the only list of the characters, and contains the functionality to add and remove lemmings among other things. Using the singleton 

Restricts the instantiation of a class to one object; useful when exactly one object is needed to coordinate actions across the system. When you want to ensure that no additional instances of the class are created accidentally.

\subsection{Levels} When it came to designing how the levels would work in my game, I wanted something which was very flexible, in that I could easily add extra levels (or modify existing levels) with a minimal amount of work. The way that I decided to do his was to create a number of generic assets which I could re-use in each level. Things such as platforms, water hazards, trapdoors etc. This meant that when I wanted to add an additional level to the game, I could simply specify the positioning of these existing elements, rather than having to create new level assets per level. By using re-usable assets, not only was adding new levels a lot simpler, it also meant that I could drastically cut down on the amount of disk-space that my game needed.

To accompany the assets, I also needed some way to have some way to specify the layout of each level, as well as other level specific data. To accomplish this, I used the natively supported Property List (plist) file format. Plist files are simply XML files of dictionary objects; consisting of a key object (usually a string), and a 'value' object. One of the advantages to using the plist is that it's easily editable in any text editor. It also supports a variety of datatypes, such as strings, numbers (ints, floats, etc) and dates. You can also nest dictionaries inside each other for even more flexibility. 

I used one plist file to specify how many levels there were in my game, along with certain level-specific information such as difficulty, number of tool uses. I then created a plist file for each level, which stored the positions of each piece of terrain, as well as the type of the terrain, and in some cases whether the terrain piece was collide able (this was enabled by default, so was only included if the object was to be removed from collision detection). My game then loaded in the list of the levels, and each individual level plist. 

Initially, I planned to have levels of varying difficulty, with an option on the new game screen to allow the user to select a difficulty. However, I found it difficult to determine what an 'easy' level should be for example, and so the final game just assumes all of the levels are the same difficulty.

\subsection{Basic Functionality} Agent behaviour (non-AI), winning the game

\subsection{Constants} I created a file to contain the constants used in my program (for example the framerate, the default font filename and the various rewards associated with reinforcement learning). Moving all of these variables into a single separate file means that Each constant in this file follows the standard naming convention for constants (i.e. starting with a "k"). This is a legacy feature from the pre-Mac OS X days, possibly back when the Mac OS was written mostly in Pascal. It's interesting that this convention is kept up, given that it doesn't follow the standard C convention of defining constants in capital letters. 

\subsection{Datatypes} Similar to the constants file. Defines all of the datatypes/enums used in my game.

\subsection{Utility Methods} I created a Utils class with some useful static-access 'utility' methods which I was likely to want to use a lot in my game; functions such as random number generation, enumeration-to-string conversions, and timestamp generation. It made sense to create a separate class with all of these types of methods because it not only cut down on the amount of code that I had to write, but also meant that were I to find a bug in my random number generation code for example, I would only need to fix the bug in one place, rather than having to go and search every source file for occurrences of the broken code. 
		
\subsection{Deviance From The Design} Overall, I think I adhered to my original designs pretty 

\subsection{Performance Optimisations} As already mentioned, Cocos2D offers a variety of performance optimisations specifically tailored for developing games. One such optimisation is the 'CCSpriteBatchNode'. It is not uncommon to experience poor performance when there are a large number of objects onscreen at once. 

For each sprite, OpenGL ES must first bind to the texture, and then render the sprite. As more and more sprites are added to the screen, it isn't difficult to see that the number of calls to OpenGL will steeply increase too, with every call costing a few CPU cycles. It is basic common sense to see that the game will run faster the fewer calls to OpenGL are made. 

The CCSpriteBatchNode is a Cocos2D class which has been created to help with this problem. The CCSpriteBatchNode works by taking a single texture which contains all of the textures needed by the current scene (called a texture atlas, SEE DIAGRAM), and sends all of the images for rendering to OpenGL at once, rather than individually. This essentially reduces the number of bind calls needed from O(n) to O(1) (in Big-O notation). Using texture atlases also cuts down on the amount of memory needed to store each individual image. In some version of the iOS, textures were required to be stored in sizes of the poet of two (64, 128, 512 etc.). This meant that the textures would often have a lot of unnecessary white space around the image (SEE DIAGRAM), which obviously takes up extra memory; something which even with todays devices is not a commodity. An additional optimisation which can be made regarding the textures is to use the 	compressed PVR CCZ file format. In addition to saving disk space in terms of the actual size of the image, the PVR CCZ format is supported natively by the iPhone's GPU, and can be loaded directly onto the GPU without the need for conversion.

Aside from the performance optimisations offered by Cocos2D, I also did a few other things to ensure that my program ran as efficiently as possible.

\subsection{Random number generation} Although it may not seem like a likely source for a performance bottleneck, the random number generator (RNG) I chose to use could have a noticeable impact on performance. Based on some very quick tests carried out by a poster on the official Cocos2D forums (http://www.cocos2d-iphone.org/forum/topic/11290), I was surprised to see the difference in speed between different RNGs. SEE TABLE. Typically, the RNG I had chosen to use (arc4Rand) was by far the slowest method tested. Based on the tests carried out, I instead decided to use the CCRANDOM\_0\_1 function, which was shown to be 5 times quicker. Although the tests carried out on the forum were tested by performing 5 million iterations of the algorithm, far more than I would ever need to use, every performance enhancement helps.
		
\subsection{Problems Faced} General problems learning the language, differences between languages learnt already. Can only add objects to an array - need to use [NSNull null]

An array can contain only objects (same as Java, but Java hides this from the user by wrapping core types in objects). Similar fix in Objective-C, but have to do it manually (e.g. [NSNumber numberWithInt:1]).

\subsection{Memory management} Something which I found particularly tricky about using Objective-C over a language like Java was the memory management. It isn't until you start to learn another language that you realise how much the Java runtime shields the programmer from the low-level bugs associated with memory management thanks to the garbage collector. In Java, memory is reserved for an object simply by using the 'new' keyword, and memory is automatically released when that object goes out of scope and is no longer accessible. This could be by the programmer setting the object to null. At this point, the garbage collector reclaims the memory.

On the other hand, languages such as Objective-C  require manual memory management, meaning that every object created needs to be manually released by the programmer when finished. 

The big problem with memory leaks is that they can often go undetected, only to cause the program to unexpectedly crash later on down the line. For this reason, they can be notoriously difficult to track down. 

I discovered a particularly nasty bug in my game's update method for example, which is called every frame, whereby I was allocating an array to hold all of the game objects, but failing to release the memory at the end of the method. 

I discovered that I had made this very error in my game's main update method (which is called 60 times per second). Using Instruments, I found that this relatively minor error was losing a lot of memory, and causing the game to drop 15 frames per second after about 15 minutes. Although this may not seem like much, it had a knock-on effect on my game timer, which determined the game time by counting each frame. Therefore, losing 15fps meant that I was essentially losing one second in every five. 

Carrying out this project really taught me the importance of keeping a close eye on what memory you're allocating and where you're releasing it. What may at first seem like a negligible memory leak can build up over time, and combined with other similarly small memory leaks, can cause big problems with your programs performance. I would even go so far as to say that these smaller memory leaks are even more troublesome than the bigger ones, as they are much harder to track down, and are often more widely spread, and therefore more difficult to fix.

Xcode comes with a very useful set of performance analysis tools called 'Instruments' which can be used to establish any memory-related or runtime issues, such as memory leaks etc.

Another problem I frequently came across which couldn't happen in Java was that of initialised variables. Whereas in Java, whenever you declare a new variable, it's automatically initialised with whatever the default value may be, you get no such privilege in Objective-C. When you declare a variable in Objective-C, it stays uninitialised until you specifically set it. This caused me the biggest problems with arrays; trying to access an uninitialised array wouldn't cause a compiler error, but would cause a crash at runtime.


\section{OpenGL ES} The Open Graphics Library for Embedded Systems (OpenGL ES) is a subset of the OpenGL 3D graphics API which targets embedded systems such as the Nintendo 3DS, the Sony Playstation 3, and devices running Apple's iOS and Google's Android operating systems.

\section{Cocos2D} The Cocos2D framework uses a hierarchical structure to manage the many components which make up a typical game. It uses the analogy of film production to make this easy to understand. At the highest level is the CCDirector class which, as one would expect, controls the running of each individual level in the game (known as CCScenes). Each scene consists of one or more  layers (CCLayers) which contain the actual game objects (for example the background image, the level terrain and the game characters). The game objects could take the form of CCSprites (at it's most basic terms, a sprite is an image displayed onscreen)

The Cocos2D framework is loaded via the AppDelegate singleton class, a class which deals with the basic running of the application; what should happen when launching the app, when the user closes the app, when the app is sent to the background etc.

In computer programming, a constant is an identifier whose associated value cannot typically be altered by the program during its execution.

\chapter{AI Component}

\section{Code Structure/Features}

\section{Deviance From The Design}
	
\section{Performance Optimisations}

\section{Problems Faced}
	
%
% Section Page
%
\part{Testing \& Evaluation}{This section presents the usability and learning testing I performed, the results that were obtained, and an analysis of the product's performance.}

\chapter{Testing \& Results}

Show the results of usability testing (if any) and the learning results. Compare with the different types and no learning, show graphs to compare. Discuss the effectiveness, advantages/disadvantages to the learning algorithms used.

\section{Usability Testing}

\section{AI Testing}

\subsection{Analysis of Decision Tree Learning}
\subsection{Analysis of Reinforcement Learning}

%
% Section Page
%
\chapter{Evaluation}

A very competent evaluation of the whole project (with hindsight). Weak performance in the evaluation of product and process will result in a grade no higher than a B.

\section{Time Management}

Initial plan was to use a waterfall development approach (discuss). Eventually took a slightly more incremental approach: developed game and AI together towards the end, some asset creation throughout. Discuss why, which would have been better - for one person rather than a team. As I was unfamiliar with the language and the engine, it was difficult to decide how best to approach the design/implementation. (see \url{http://en.wikipedia.org/wiki/Software_development_methodology})
		
- Waterfall: a linear framework
- Incremental: a combined linear-iterative framework
- Spiral: a combined linear-iterative framework
		
The biggest miscalculation with regards to my time-management and project schedule was that I failed to recognise just how much groundwork was needed to build the game component. Though I by no means expected the game development to be a trivial task, I wasn't prepared for the amount of work that was needed. This was exacerbated by the fact that I was also still getting to grips with Objective-C, and so my productivity wasn't as it could have been. However, although I slipped by a couple of weeks during the game development stage, I finished the AI component of the game under-schedule. Looking back at the volume of code that was needed to get the game running, it seems fairly that I should have at least allocated an equal amount of time to both the game and AI components. As it happens, I still finished my project to schedule with some time to spare.

\section{Achievements}

\section{Evaluation of reinforcement learning}

+ it can handle uncertain and noisy domains
+ it can interface with an external world more easily than other methods
- slow learning rate as they rely on backwards propagation of rewards. The problem solver must pass through early parts of the problem space many times before any rewards at all reach those locales.


\section{Areas For Enhancement}

Although I'm very pleased with the outcome of my project, given extra development time, there are a number of areas which I feel I would like to extend/build upon.

The most obvious enhancement that I could make would be to extend the AI system to incorporate more learning types. Were I to do this, the next learning algorithm that I would like to try would be a genetic algorithm, as I think that it lends itself quite well to the context of my game. I could slightly alter the spawning of the lemmings so that they all spawn in waves, a certain number at a time - say 100. Then, at the end each wave, I could evaluate each lemming's performance, choosing to breed those with the most effective policy.

Another enhancement that I think it would be interesting to make would be to allow for multiple learning types to be used in a single game, with separate agents able to use different algorithms. It would be interesting to compare the performance of the two. Following on from this idea, I also think it could be interesting to mix different methods of learning together in one lemming…

Something else which I'd like to implement would be some way for the agents to learn from each other - perhaps by watching other agents. This obviously wouldn't work with the shared knowledge base, but would be interesting to see the improvement of learning this way over the original.

Another enhancement that I could make to the game would be to extend the system to a 3D environment. This would obviously complicate things, but.

- -Implement additional learning types-
- -Mixed learning types between agents-
- Use a combination of types
- Remove the random learning episodes
- Communication between agents (shared knowledge base?)
- Extend to a 3D environment
- Level editor
- "Start with a large correction factor (learning rate), and slowly decrease the parameter, which gives the algorithm a rough approximation during the early stages and allows fine tuning later in learning."
- Test alternative reinforcement methods (bucket brigade, temporal difference)
- Negatives of using reinforcement learning: it doesn't generalise beyond specific states. Possible fix: use genetic algorithms - "the expected reward on production rules serves as their fitness, with more fit rules being selected for reproduction. The new rules are then evaluated by running them in an environment, with the best of them being selected to produce another generation, and so on. Such classifier systems are often used in conjunction with the bucket brigade algorithm for determining expected rewards." 

\section{Overall Success}
- Applications For Learning in games (possibly pre-release learning)
- Performance on mobile device

%
% Section Page
%
\part{Appendices}{}

	\appendix
	
	\chapter{Code Listings}

	\chapter{Project Log}
	
A day-by-day work diary kept by you as the project is carried out. A log which has evidently been manufactured to be handed in rather than having been properly kept will result in a lower grade.

	\chapter{Git Log}
	
% push the references onto a new page
\newpage
\bibliographystyle{abbrv}
\bibliography{Cogito}

\end{document}
